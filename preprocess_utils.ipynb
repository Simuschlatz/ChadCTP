{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pydicom import dcmread\n",
    "from icecream import ic\n",
    "from skimage import morphology\n",
    "from scipy import ndimage\n",
    "from skimage.measure import label, find_contours\n",
    "import pickle\n",
    "from numba import jit\n",
    "from scipy import ndimage\n",
    "from skimage.restoration import denoise_nl_means\n",
    "from skimage.transform import AffineTransform\n",
    "from skimage.registration import optical_flow_tvl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dcm_datasets(folder_path: str) -> list:\n",
    "    if folder_path.endswith('.npy'):\n",
    "        return np.load(folder_path)\n",
    "    try:\n",
    "        files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.dcm')])\n",
    "        ic(f\"Dicom files loaded, count: {len(files)}\")\n",
    "\n",
    "        return [dcmread(f) for f in files] # Pydicom datasets\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Folder {folder_path} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowing_lookup = {\n",
    "    \"lung\": (-600, 1500),\n",
    "    \"mediastinum\": (50, 350),\n",
    "    \"tissues\": (50, 400),\n",
    "    \"brain\": (40, 80),\n",
    "    \"bone\": (400, 1800)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def convert_to_HU(slice, intercept, slope):\n",
    "    return slice * slope + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def normalize(scan):\n",
    "    min_val, max_val = scan.min(), scan.max()\n",
    "    return (scan - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def apply_windowing(scan, window_center, window_width):\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    return np.clip(scan, img_min, img_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_from_type(type: str):\n",
    "    try:\n",
    "        return windowing_lookup[type]\n",
    "    except KeyError:\n",
    "        ic(f\"Windowing type {type} not found in windowing lookup\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowing_params(ds):\n",
    "    return ds.WindowCenter, ds.WindowWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversion_params(ds):\n",
    "    return ds.RescaleIntercept, ds.RescaleSlope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_homogenous_windowing_params(datasets):\n",
    "    params = get_windowing_params(datasets[0])\n",
    "    return all(get_windowing_params(ds) == params for ds in datasets[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def threshold(ct_volume, skull_threshold=700):\n",
    "    \"\"\"\n",
    "    Segments the brain from the CT volume using thresholding and 3D unet\n",
    "    A fundamental step in neuroimage preprocessing \n",
    "    ``skull_threshold``: Thresholding HU value to remove skull, values from \n",
    "    https://www.sciencedirect.com/topics/medicine-and-dentistry/hounsfield-scale\n",
    "    \"\"\"\n",
    "    thresholded = np.where(ct_volume >= skull_threshold, -1000, ct_volume)\n",
    "    return thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def get_threshold_mask(slice, min_val=-50, max_val=300):\n",
    "    return np.where((min_val < slice) & (slice < max_val), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brain_mask(image: np.ndarray):\n",
    "    segmentation = morphology.dilation(image, np.ones((5, 5)))\n",
    "    labels, _ = ndimage.label(segmentation)\n",
    "    label_count = np.bincount(labels.ravel().astype(int))\n",
    "    # The size of label_count is the number of classes/segmentations found\n",
    "\n",
    "    # We don't use the first class since it's the background\n",
    "    label_count[0] = 0\n",
    "\n",
    "    # We create a mask with the class with more pixels\n",
    "    # In this case should be the brain\n",
    "    mask = labels == label_count.argmax()\n",
    "\n",
    "    # Improve the brain mask\n",
    "    mask = morphology.dilation(mask, np.ones((5, 5)))\n",
    "    mask = ndimage.morphology.binary_fill_holes(mask)\n",
    "    mask = morphology.dilation(mask, np.ones((3, 3)))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rigid_registration(volume_sequence):\n",
    "    \"\"\"\n",
    "    Performs rigid registration on a sequence of CT brain volumes.\n",
    "    \n",
    "    This function first denoises the volumes and then applies rigid registration\n",
    "    to align all volumes in the sequence with the first volume.\n",
    "    \n",
    "    Parameters:\n",
    "    - volume_sequence (numpy.ndarray): 4D array of CT volumes (time, z, y, x)\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: Registered 4D volume sequence\n",
    "    \"\"\"\n",
    "    # Denoise each volume in the sequence\n",
    "    denoised_sequence = np.zeros_like(volume_sequence)\n",
    "    for t in range(volume_sequence.shape[0]):\n",
    "        denoised_sequence[t] = denoise_nl_means(volume_sequence[t], h=0.1, fast_mode=True, patch_size=5, patch_distance=6)\n",
    "    \n",
    "    # Use the first volume as the reference\n",
    "    reference = denoised_sequence[0]\n",
    "    registered_sequence = np.zeros_like(denoised_sequence)\n",
    "    registered_sequence[0] = reference\n",
    "    \n",
    "    # Perform rigid registration for each subsequent volume\n",
    "    for t in range(1, denoised_sequence.shape[0]):\n",
    "        # Estimate transformation using optical flow\n",
    "        flow = optical_flow_tvl1(reference, denoised_sequence[t])\n",
    "        \n",
    "        # Convert flow to affine transformation\n",
    "        affine = AffineTransform(translation=(-flow[0].mean(), -flow[1].mean()))\n",
    "        \n",
    "        # Apply transformation\n",
    "        registered_sequence[t] = ndimage.affine_transform(denoised_sequence[t], affine.params)\n",
    "    \n",
    "    return registered_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brain(slice):\n",
    "    slice = get_threshold_mask(slice) * slice\n",
    "    slice = apply_windowing(slice, 40, 80)\n",
    "    mask = get_brain_mask(slice)\n",
    "    return slice * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def genCont(image, cont):\n",
    "    \"\"\"Function to create image contour from coordinates\"\"\"\n",
    "    cont_imag = np.zeros_like(image)\n",
    "    for ii in range(len(cont)):\n",
    "        cont_imag[cont[ii,0],cont[ii,1]] = 1\n",
    "    return cont_imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skull_strip_mask(image: np.ndarray, bone_hu=110, ct_inf=-110, ct_sup=120):\n",
    "    \"\"\"\n",
    "    Creates a skull-stripping mask for CT images.\n",
    "\n",
    "    This function generates a binary mask to separate the brain tissue from the skull\n",
    "    and other non-brain structures in CT images. It uses a combination of thresholding,\n",
    "    morphological operations, and contour analysis to identify and isolate the brain region.\n",
    "\n",
    "    Parameters:\n",
    "    - image (numpy.ndarray): The input CT image.\n",
    "    - bone_hu (int): The Hounsfield Unit threshold for bone, default is 110.\n",
    "    - ct_inf (int): The lower Hounsfield Unit threshold for brain tissue, default is -110.\n",
    "    - ct_sup (int): The upper Hounsfield Unit threshold for brain tissue, default is 120.\n",
    "\n",
    "    Returns:\n",
    "    - mask (numpy.ndarray): The binary skull-stripping mask.\n",
    "\n",
    "    Note:\n",
    "    This method assumes that the input image is a 2D slice from a CT volume.\n",
    "    The resulting mask can be applied to the original image to remove non-brain structures.\n",
    "    \"\"\"\n",
    "    img_max = np.max(image)\n",
    "    # Selecting areas with certain\n",
    "    image_mask = np.zeros_like(image).astype(int)\n",
    "    image_mask[(bone_hu < image) & (image < img_max)] = 1\n",
    "    # Removing objects with area smaller than a certain values\n",
    "    image_mask_clean = morphology.remove_small_objects(image_mask.astype(bool), 1500)\n",
    "\n",
    "    # Improving skull definition\n",
    "    labels, label_nb = ndimage.label(image_mask_clean)\n",
    "    se = morphology.disk(10)\n",
    "    close_small_bin = morphology.closing(labels, se)\n",
    "    # Finding contours of the various areas\n",
    "    contours = find_contours(close_small_bin,0)\n",
    "\n",
    "    # Creating masks of the various rounded areas\n",
    "    areas = []\n",
    "    masks = []\n",
    "    for contour in contours:\n",
    "        cont = genCont(image_mask_clean,np.array(contour,dtype=int)).astype(int)\n",
    "        mask = morphology.dilation(cont, np.ones((2, 2)))\n",
    "        mask = ndimage.morphology.binary_fill_holes(mask)\n",
    "        mask = morphology.dilation(mask, np.ones((3, 3)))\n",
    "        masks.append(mask.copy())\n",
    "        # Computing areas to find correct inner portion\n",
    "        areas.append(np.sum(mask.ravel()))\n",
    "\n",
    "    if len(areas) == 1:\n",
    "        # If only one contour is found, there is no inner portion\n",
    "        mask = masks[0].astype(np.float32)\n",
    "    elif len(areas) > 1:\n",
    "        # If two or more contours have been found, take the second-largest one as inner portion\n",
    "        sort_idx = np.argsort(areas)\n",
    "        mask = masks[sort_idx[1]].astype(np.float32)\n",
    "\n",
    "        # Improving skull definition\n",
    "        maskedImg = image * mask\n",
    "        image_mask = np.zeros_like(maskedImg).astype(int)\n",
    "        image_mask[(ct_inf < maskedImg) & (maskedImg < ct_sup)] = 1\n",
    "        # Removing objects with area smaller than a certain values\n",
    "        image_mask_clean = morphology.remove_small_objects(image_mask.astype(bool), 3000)\n",
    "\n",
    "        # Improving skull definition again\n",
    "        labels, label_nb = ndimage.label(image_mask_clean)\n",
    "        se = morphology.disk(2)\n",
    "        close_small_bin = morphology.closing(labels, se)\n",
    "        if close_small_bin.max() == 2:\n",
    "            image_mask = np.zeros_like(maskedImg).astype(int)\n",
    "            image_mask[close_small_bin == 2] = 1\n",
    "            image_mask_clean = morphology.remove_small_objects(image_mask.astype(bool), 6000)\n",
    "            mask = ndimage.morphology.binary_fill_holes(image_mask_clean)\n",
    "    else:\n",
    "        # No areas have been found\n",
    "        mask = np.zeros_like(image_mask_clean, dtype=np.float32)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def efficient_downsample(volume_seq: np.ndarray, factor=4):\n",
    "    return volume_seq[::factor, ::factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volume(folder_path, windowing=True, extract_brain=True, spatial_downsampling_factor=4, temporal_downsampling_factor=1) -> np.ndarray:\n",
    "    datasets = load_dcm_datasets(folder_path)\n",
    "    if isinstance(datasets, np.ndarray):\n",
    "        return datasets\n",
    "    \n",
    "    # Otherwise process the DICOM files\n",
    "    # Each file contains the entire perfusion volume sequence as DICOM datasets\n",
    "    # The objective is to convert the sequence into a 4D array of CT volumes that are\n",
    "    # in HU, windowed, brain-extracted, normalized, registered and filtered\n",
    "\n",
    "    ds = datasets[0]\n",
    "    params = {\n",
    "    \"window_level\": ds.WindowCenter,\n",
    "    \"window_width\": ds.WindowWidth,\n",
    "    \"slice_thickness\": ds.SliceThickness,\n",
    "    }\n",
    "    # Assume that each volume in the sequence has the same dimensions\n",
    "    Y = int((datasets[-1].SliceLocation - datasets[0].SliceLocation + 5) // ds.SliceThickness) # Height\n",
    "    Z, X = ds.Rows // spatial_downsampling_factor, ds.Columns // spatial_downsampling_factor # Depth, Width\n",
    "    T = len(datasets) // Y // temporal_downsampling_factor # Temporal dimension\n",
    "    volume_seq = np.empty((T, Y, Z, X), dtype=np.float32)\n",
    "\n",
    "    metadata = {} # Metadata for the volume\n",
    "    positional_params = []\n",
    "\n",
    "    if windowing:\n",
    "        window_center, window_width = get_window_from_type('brain')\n",
    "    ic(window_center, window_width)\n",
    "\n",
    "    for i, ds in enumerate(datasets):\n",
    "        slice = ds.pixel_array\n",
    "        slice = efficient_downsample(slice, factor=spatial_downsampling_factor)\n",
    "        slice = convert_to_HU(slice, *get_conversion_params(ds))\n",
    "        sequence_index, slice_index = divmod(i, Y)\n",
    "        volume_seq[sequence_index, slice_index] = slice\n",
    "\n",
    "    if windowing:\n",
    "        volume_seq = apply_windowing(volume_seq, window_center, window_width)\n",
    "\n",
    "    return volume_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_volume(volume, folder_path='volume.npy'):\n",
    "    np.save(folder_path, volume)\n",
    "    ic(f\"Volume saved to {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def bilateral_filter(volume_seq, sigma_space, sigma_intensity):\n",
    "    def gaussian(x_square, sigma):\n",
    "        return np.exp(-0.5 * x_square / sigma**2)\n",
    "    \n",
    "    kernel_size = int(2 * sigma_space + 1)\n",
    "    half_kernel_size = int(kernel_size / 2)\n",
    "    time, depth, height, width = volume_seq.shape\n",
    "    result = np.zeros_like(volume_seq)\n",
    "    W = np.zeros_like(volume_seq)\n",
    "\n",
    "    for t in range(time):\n",
    "        for k in range(depth):\n",
    "            for i in range(height):\n",
    "                print(t, k, i)\n",
    "                for j in range(width):\n",
    "                    for z in range(-half_kernel_size, half_kernel_size + 1):\n",
    "                        for y in range(-half_kernel_size, half_kernel_size + 1):\n",
    "                            for x in range(-half_kernel_size, half_kernel_size + 1):\n",
    "                                kk, ii, jj = k + z, i + y, j + x\n",
    "                                if 0 <= kk < depth and 0 <= ii < height and 0 <= jj < width:\n",
    "                                    Gspace = gaussian(x**2 + y**2 + z**2, sigma_space)\n",
    "                                    intensity_diff = volume_seq[t, k, i, j] - volume_seq[t, kk, ii, jj]\n",
    "                                    Gintensity = gaussian(intensity_diff**2, sigma_intensity)\n",
    "                                    weight = Gspace * Gintensity\n",
    "                                    result[t, k, i, j] += weight * volume_seq[t, kk, ii, jj]\n",
    "                                    W[t, k, i, j] += weight\n",
    "\n",
    "    return result / W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.expanduser('~/Desktop/UniToBrain')\n",
    "def save_folder_paths(output_file: str='folder_paths.pkl'):\n",
    "    folder_paths = []\n",
    "    for folder in sorted(os.listdir(dataset_path)):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if len(folder) == 7 and len(os.listdir(folder_path)) == 288: # MOL-XYZ\n",
    "            folder_paths.append(folder_path)\n",
    "    ic(len(folder_paths), folder_paths[len(folder_paths)-10:])\n",
    "\n",
    "    output_file = os.path.join(dataset_path, output_file)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(folder_paths, f)\n",
    "\n",
    "    ic(f\"Folder paths saved to {output_file}\")\n",
    "    return folder_paths\n",
    "\n",
    "def load_folder_paths(output_file: str='folder_paths.pkl'):\n",
    "    file_path = os.path.join(dataset_path, output_file)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {output_file} does not exist, running save_folder_paths() instead...\")\n",
    "        return save_folder_paths(output_file)\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "folder_paths = load_folder_paths()\n",
    "print(len(folder_paths))\n",
    "folder_path = folder_paths[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f\"Dicom files loaded, count: {len(files)}\": 'Dicom files loaded, count: 288'\n",
      "ic| window_center: 40, window_width: 80\n",
      "ic| f\"Dicom files loaded, count: {len(files)}\": 'Dicom files loaded, count: 288'\n",
      "ic| window_center: 40, window_width: 80\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "v_raw = get_volume(folder_path, spatial_downsampling_factor=1)\n",
    "reg_filtered_path = folder_path + '_Registered_Filtered_3mm_20HU'\n",
    "v_registered_filtered = get_volume(reg_filtered_path, spatial_downsampling_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_raw = efficient_downsample(v_raw, factor=4)[:1, :1, :1]\n",
    "v_raw_filtered = bilateral_filter(v_raw, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| axes: array([<Axes: >, <Axes: >], dtype=object)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x179f12d40>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "from visualize import interactive_plot, multiple_interactive_plots, multi_volume_interactive_plot\n",
    "multi_volume_interactive_plot([v_raw, v_registered_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
