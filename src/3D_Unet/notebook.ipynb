{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(path: str):\n",
    "    return np.load(path)\n",
    "\n",
    "v1 = load_data('../selected_volumes/MOL-001.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import IntSlider, interact\n",
    "def multi_vol_seq_interactive(volume_seqs, titles=None):\n",
    "    \"\"\"\n",
    "    Interactive plot of multiple volume sequences using ipywidgets\n",
    "    \n",
    "    Parameters:\n",
    "    - volume_seqs: List of 4D volume sequences to display\n",
    "    - titles: Optional list of titles for each sequence\n",
    "    \"\"\"\n",
    "    if titles is None:\n",
    "        titles = [f\"Volume {i+1}\" for i in range(len(volume_seqs))]\n",
    "        \n",
    "    num_volumes = len(volume_seqs)\n",
    "    nrows = int(num_volumes ** 0.5)\n",
    "    ncols = (num_volumes + nrows - 1) // nrows\n",
    "    \n",
    "    def plot_volumes(time_idx, slice_idx):\n",
    "        fig, axes = plt.subplots(nrows, ncols, \n",
    "                                figsize=(5*ncols, 5*nrows),\n",
    "                                squeeze=True)\n",
    "        if nrows == 1:\n",
    "            if ncols == 1:\n",
    "                axes = [[axes]]\n",
    "            else:\n",
    "                axes = [axes]\n",
    "                \n",
    "        for i, (volume_seq, title) in enumerate(zip(volume_seqs, titles)):\n",
    "            row, col = i // ncols, i % ncols\n",
    "            ax = axes[row][col]\n",
    "            \n",
    "            t = min(time_idx, len(volume_seq) - 1)\n",
    "            s = min(slice_idx, len(volume_seq[t]) - 1)\n",
    "            \n",
    "            im = ax.imshow(volume_seq[t][s], cmap='magma')\n",
    "            ax.set_title(title)\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show(block=True)\n",
    "        \n",
    "    max_time = max(len(vol) for vol in volume_seqs) - 1\n",
    "    max_slice = max(len(vol[0]) for vol in volume_seqs) - 1\n",
    "    \n",
    "    interact(\n",
    "        plot_volumes,\n",
    "        time_idx=IntSlider(min=0, max=max_time, step=1, value=0, description='Time:'),\n",
    "        slice_idx=IntSlider(min=0, max=max_slice, step=1, value=0, description='Slice:')\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d983b0be22a42e599cefa21b97aefd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Time:', max=17), IntSlider(value=0, description='Slice:'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_vol_seq_interactive([v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 16, 256, 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1:\n",
    "- load the data as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset3D(Dataset):\n",
    "    def __init__(self, data_paths, context_window=4, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.context_window = context_window\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        # For every path to a volume sequence in .npy\n",
    "        for data_path in self.data_paths:\n",
    "            volume_seq = np.load(data_path)\n",
    "            # Convert to tensor\n",
    "            volume_seq = torch.from_numpy(volume_seq)\n",
    "            # Generate samples\n",
    "            for i in range(len(volume_seq) - self.context_window):\n",
    "                # Input volume sequence (context_window x 16 x 256 x 256), target volume (16 x 256 x 256)\n",
    "                self.samples.append((volume_seq[i:i+self.context_window], volume_seq[i+self.context_window]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../selected_volumes'\n",
    "device = torch.device('mps') or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Training parameters\n",
    "batch_size = 4\n",
    "sequence_length = 4\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "# Data parameters\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "\n",
    "\n",
    "data_paths = [os.path.join(root, path) for path in os.listdir(root)]\n",
    "dataset = Dataset3D(data_paths, context_window=4)\n",
    "\n",
    "def get_data_loaders(batch_size=4, sequence_length=4):\n",
    "    # Load all folder paths\n",
    "    # Split into train/val/test\n",
    "    n_train = int(len(data_paths) * train_split)\n",
    "    n_val = int(len(data_paths) * val_split)\n",
    "    \n",
    "    train_paths = data_paths[:n_train]\n",
    "    val_paths = data_paths[n_train:n_train+n_val]\n",
    "    test_paths = data_paths[n_train+n_val:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Dataset3D(train_paths, sequence_length)\n",
    "    val_dataset = Dataset3D(val_paths, sequence_length)\n",
    "    test_dataset = Dataset3D(test_paths, sequence_length)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders(batch_size=batch_size, sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "1 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "2 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "3 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "4 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "5 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "6 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "7 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "8 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "9 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "10 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "11 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "12 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "13 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "14 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "15 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "16 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "17 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "18 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "19 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "20 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "21 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "22 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "23 torch.Size([4, 4, 16, 256, 256]) torch.Size([4, 16, 256, 256])\n",
      "24 torch.Size([2, 4, 16, 256, 256]) torch.Size([2, 16, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i, (input, target) in enumerate(train_loader):\n",
    "    print(i, input.shape, target.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2+1)D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class TemporalAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, sequence_length=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm([dim, None, None])  # Normalize over channels\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time, channels, height, width]\n",
    "        b, t, c, h, w = x.shape\n",
    "        \n",
    "        # Reshape for attention\n",
    "        x = x.permute(0, 3, 4, 1, 2)  # [B, H, W, T, C]\n",
    "        x = x.reshape(b*h*w, t, c)     # [B*H*W, T, C]\n",
    "        \n",
    "        # Apply attention\n",
    "        x = self.norm(x)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Reshape back\n",
    "        x = attn_out.reshape(b, h, w, t, c)\n",
    "        x = x.permute(0, 3, 4, 1, 2)   # [B, T, C, H, W]\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class PerfusionCTPredictor(nn.Module):\n",
    "    def __init__(self, input_frames=4, base_filters=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_frames = input_frames\n",
    "        \n",
    "        # Encoder (single channel for CT scans)\n",
    "        self.enc1 = DoubleConv2D(1, base_filters)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = DoubleConv2D(base_filters, base_filters*2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck with temporal attention\n",
    "        self.bottleneck_conv = DoubleConv2D(base_filters*2, base_filters*4)\n",
    "        self.temporal_attention = TemporalAttentionBlock(\n",
    "            dim=base_filters*4,\n",
    "            num_heads=4,  # Reduced for sequence length of 4\n",
    "            sequence_length=input_frames\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv2 = nn.ConvTranspose2d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv2D(base_filters*4, base_filters*2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv2D(base_filters*2, base_filters)\n",
    "        \n",
    "        # Final prediction head\n",
    "        self.final_conv = nn.Conv2d(base_filters, 1, kernel_size=1)\n",
    "        \n",
    "        # Optional: Intensity normalization\n",
    "        self.instance_norm = nn.InstanceNorm2d(1, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time=4, height, width]\n",
    "        b, t, h, w = x.shape\n",
    "        assert t == self.input_frames, f\"Expected {self.input_frames} input frames, got {t}\"\n",
    "        \n",
    "        # Optional: Normalize intensities\n",
    "        x = x.view(b*t, 1, h, w)\n",
    "        x = self.instance_norm(x)\n",
    "        x = x.view(b, t, h, w)\n",
    "        \n",
    "        # Process each time step through encoder\n",
    "        encoder_features = []\n",
    "        bottleneck_features = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            # Encoder path\n",
    "            x1 = self.enc1(x[:, i].unsqueeze(1))  # Add channel dimension\n",
    "            x2 = self.enc2(self.pool1(x1))\n",
    "            \n",
    "            encoder_features.append((x1, x2))\n",
    "            \n",
    "            # Bottleneck\n",
    "            bottle = self.bottleneck_conv(self.pool2(x2))\n",
    "            bottleneck_features.append(bottle)\n",
    "            \n",
    "        # Stack and apply temporal attention at bottleneck\n",
    "        bottleneck_features = torch.stack(bottleneck_features, dim=1)\n",
    "        bottleneck_features = self.temporal_attention(bottleneck_features)\n",
    "        \n",
    "        # Use the last temporal feature for prediction\n",
    "        bottle = bottleneck_features[:, -1]  # Take last temporal state\n",
    "        \n",
    "        # Decoder path (single time step)\n",
    "        x1, x2 = encoder_features[-1]  # Use features from last input frame\n",
    "        \n",
    "        d2 = self.upconv2(bottle)\n",
    "        d2 = torch.cat([d2, x2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, x1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # Predict next frame\n",
    "        pred = self.final_conv(d1)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "# Training setup\n",
    "def train_perfusion_predictor():\n",
    "    model = PerfusionCTPredictor(input_frames=4)\n",
    "    criterion = nn.MSELoss()  # or nn.L1Loss()\n",
    "    \n",
    "    # Optional: Add Huber loss for robustness to outliers\n",
    "    huber_loss = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    def train_step(x, y):\n",
    "        # x: [batch, 4, height, width]\n",
    "        # y: [batch, 1, height, width] (next frame)\n",
    "        pred = model(x)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss_mse = criterion(pred, y)\n",
    "        loss_huber = huber_loss(pred, y)\n",
    "        loss = loss_mse + 0.5 * loss_huber\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D UNet with temporal Attention Block in Bootleneck"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
