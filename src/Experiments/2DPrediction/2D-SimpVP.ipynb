{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnbennewiz\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from ipywidgets import IntSlider, interact\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint\n",
    "from pytorch_lightning.tuner import Tuner\n",
    "\n",
    "import kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 100\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=100):\n",
    "    seed_everything(seed, workers=True)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For all GPUs\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from rich.progress import track\\nload_root=\"../QualityFiltered\"\\nsave_root=\"../NormalizedQualityFiltered\"\\nfor volume_name in track([path for path in os.listdir(root)]):\\n    #load\\n    volume_seq = np.load(os.path.join(load_root, volume_name))\\n    #min-max scaling\\n    volume_seq = (volume_seq - volume_seq.min())/(volume_seq.max() - volume_seq.min())\\n    #save\\n    np.save(os.path.join(save_root, volume_name), volume_seq)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small function for normalizing the already standardized data\n",
    "\"\"\"from rich.progress import track\n",
    "load_root=\"../QualityFiltered\"\n",
    "save_root=\"../NormalizedQualityFiltered\"\n",
    "for volume_name in track([path for path in os.listdir(root)]):\n",
    "    #load\n",
    "    volume_seq = np.load(os.path.join(load_root, volume_name))\n",
    "    #min-max scaling\n",
    "    volume_seq = (volume_seq - volume_seq.min())/(volume_seq.max() - volume_seq.min())\n",
    "    #save\n",
    "    np.save(os.path.join(save_root, volume_name), volume_seq)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2D(Dataset):\n",
    "    def __init__(\n",
    "        self, data_paths, context_window=4, prediction_window=1, transform=None\n",
    "    ):\n",
    "        self.data_paths = data_paths\n",
    "        self.context_window = context_window\n",
    "        self.prediction_window = prediction_window\n",
    "        self.transform = transform\n",
    "        self.depth = None\n",
    "        # For every path to a volume sequence in .npy\n",
    "        self.data_attributes = []\n",
    "        test = np.load(data_paths[0])\n",
    "        for data_path in self.data_paths:\n",
    "            for h in range(test.shape[1]):\n",
    "                for t in range(len(test)-self.context_window - self.prediction_window+1):\n",
    "                    # file_path, t, h\n",
    "                    self.data_attributes.append([data_path, t, h])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_attributes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.data_attributes[idx][1]\n",
    "        h = self.data_attributes[idx][2]\n",
    "        volume_seq = torch.from_numpy(np.load(self.data_attributes[idx][0]))\n",
    "        return (\n",
    "            volume_seq[t:t+self.context_window, h].unsqueeze(1),\n",
    "            volume_seq[t+self.context_window:t+self.context_window+self.prediction_window, h].unsqueeze(1)\n",
    "        )\n",
    "\n",
    "class Dataset3D(Dataset):\n",
    "    def __init__(self, data_paths, context_window=4, prediction_window=1, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.context_window = context_window\n",
    "        self.prediction_window = prediction_window\n",
    "        self.transform = transform\n",
    "\n",
    "        self.data_attributes = []\n",
    "        test = np.load(self.data_paths[0])\n",
    "        for data_path in self.data_paths:\n",
    "            for t in range(test.shape[0]-self.context_window-self.prediction_window+1):\n",
    "                # file_path, t\n",
    "                self.data_attributes.append([data_path, t])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_attributes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.data_attributes[idx][1]\n",
    "        volume_seq = torch.from_numpy(np.load(self.data_attributes[idx][0]))\n",
    "        return (\n",
    "            volume_seq[t:t+self.context_window].unsqueeze(1), \n",
    "            volume_seq[t+self.context_window:t+self.context_window+self.prediction_window].unsqueeze(1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    \"\"\"Ensures worker processes get the same seed\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class VolumeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, root, batch_size=4, sequence_length=4, prediction_length=1, num_workers=0, drop_last=False, pin_memory=False, train_split=0.8, val_split=0.1, test_split=0.1):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.pin_memory = pin_memory\n",
    "        \n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "        \n",
    "        self.train_paths = None\n",
    "        self.val_paths = None\n",
    "        self.test_paths = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_paths = [os.path.join(self.root, path) for path in os.listdir(self.root)]\n",
    "        total_size = len(data_paths)\n",
    "    \n",
    "        # Normalize splits if they don’t sum to 1\n",
    "        split_sum = self.train_split + self.val_split + self.test_split\n",
    "        if split_sum != 1.0:\n",
    "            self.train_split /= split_sum\n",
    "            self.val_split /= split_sum\n",
    "            self.test_split /= split_sum\n",
    "            print(f\"Normalized splits to: train={self.train_split:.2f}, val={self.val_split:.2f}, test={self.test_split:.2f}\")\n",
    "    \n",
    "        # Compute dataset sizes\n",
    "        train_size = int(total_size * self.train_split)\n",
    "        val_size = int(total_size * self.val_split)\n",
    "        test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "    \n",
    "        # Error handling: Ensure valid split sizes\n",
    "        if train_size <= 0 or val_size <= 0 or test_size <= 0:\n",
    "            raise ValueError(f\"Invalid dataset splits: train={train_size}, val={val_size}, test={test_size}. Check your split values.\")\n",
    "    \n",
    "        # Perform random split\n",
    "        self.train_paths, self.val_paths, self.test_paths = random_split(data_paths, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        self.train_dataset = Dataset2D(self.train_paths, self.sequence_length, self.prediction_length)\n",
    "        self.val_dataset = Dataset2D(self.val_paths, self.sequence_length, self.prediction_length)\n",
    "        self.test_dataset = Dataset2D(self.test_paths, self.sequence_length, self.prediction_length)\n",
    "    \n",
    "        self.val_dataset_3d = Dataset3D(self.val_paths, self.sequence_length, self.prediction_length)\n",
    "        self.test_dataset_3d = Dataset3D(self.test_paths, self.sequence_length, self.prediction_length)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return [\n",
    "            DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker),\n",
    "            DataLoader(self.val_dataset_3d, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker),\n",
    "        ]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker)\n",
    "\n",
    "    def val_dataloader_3d(self):\n",
    "        return DataLoader(self.val_dataset_3d, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker)\n",
    "    \n",
    "    def test_dataloader_3d(self):\n",
    "        return DataLoader(self.test_dataset_3d, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker)\n",
    "\n",
    "    def teardown(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after training...\")\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after testing...\")\n",
    "\n",
    "        if stage == \"validate\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after validation...\")\n",
    "\n",
    "        # Free memory by deleting large datasets\n",
    "        del self.train_dataset\n",
    "        del self.val_dataset\n",
    "        del self.test_dataset\n",
    "        del self.val_dataset_3d\n",
    "        del self.test_dataset_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, transpose=False, act_norm=False):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.act_norm=act_norm\n",
    "        if not transpose:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,output_padding=stride //2 )\n",
    "        self.norm = nn.GroupNorm(2, out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.act(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvSC(nn.Module):\n",
    "    def __init__(self, C_in, C_out, stride, transpose=False, act_norm=True):\n",
    "        super(ConvSC, self).__init__()\n",
    "        if stride == 1:\n",
    "            transpose = False\n",
    "        self.conv = BasicConv2d(C_in, C_out, kernel_size=3, stride=stride,\n",
    "                                padding=1, transpose=transpose, act_norm=act_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GroupConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, act_norm=False):\n",
    "        super(GroupConv2d, self).__init__()\n",
    "        self.act_norm = act_norm\n",
    "        if in_channels % groups != 0:\n",
    "            groups = 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,groups=groups)\n",
    "        self.norm = nn.GroupNorm(groups,out_channels)\n",
    "        self.activate = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.activate(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, C_in, C_hid, C_out, incep_ker=[3,5,7,11], groups=8):        \n",
    "        super(Inception, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(C_in, C_hid, kernel_size=1, stride=1, padding=0)\n",
    "        layers = []\n",
    "        for ker in incep_ker:\n",
    "            layers.append(GroupConv2d(C_hid, C_out, kernel_size=ker, stride=1, padding=ker//2, groups=groups, act_norm=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y = 0\n",
    "        for layer in self.layers:\n",
    "            y += layer(x)\n",
    "        return y\n",
    "\n",
    "def stride_generator(N, reverse=False):\n",
    "    strides = [1, 2]*10\n",
    "    if reverse: return list(reversed(strides[:N]))\n",
    "    else: return strides[:N]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,C_in, C_hid, N_S):\n",
    "        super(Encoder,self).__init__()\n",
    "        strides = stride_generator(N_S)\n",
    "        self.enc = nn.Sequential(\n",
    "            ConvSC(C_in, C_hid, stride=strides[0]),\n",
    "            *[ConvSC(C_hid, C_hid, stride=s) for s in strides[1:]]\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):# B*4, 3, 128, 128\n",
    "        enc1 = self.enc[0](x)\n",
    "        latent = enc1\n",
    "        for i in range(1,len(self.enc)):\n",
    "            latent = self.enc[i](latent)\n",
    "        return latent,enc1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,C_hid, C_out, N_S):\n",
    "        super(Decoder,self).__init__()\n",
    "        strides = stride_generator(N_S, reverse=True)\n",
    "        self.dec = nn.Sequential(\n",
    "            *[ConvSC(C_hid, C_hid, stride=s, transpose=True) for s in strides[:-1]],\n",
    "            ConvSC(2*C_hid, C_hid, stride=strides[-1], transpose=True)\n",
    "        )\n",
    "        self.readout = nn.Conv2d(C_hid, C_out, 1)\n",
    "    \n",
    "    def forward(self, hid, enc1=None):\n",
    "        for i in range(0,len(self.dec)-1):\n",
    "            hid = self.dec[i](hid)\n",
    "        Y = self.dec[-1](torch.cat([hid, enc1], dim=1))\n",
    "        Y = self.readout(Y)\n",
    "        return Y\n",
    "\n",
    "class Mid_Xnet(nn.Module):\n",
    "    def __init__(self, channel_in, channel_hid, N_T, incep_ker = [3,5,7,11], groups=8):\n",
    "        super(Mid_Xnet, self).__init__()\n",
    "\n",
    "        self.N_T = N_T\n",
    "        enc_layers = [Inception(channel_in, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            enc_layers.append(Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "        enc_layers.append(Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "\n",
    "        dec_layers = [Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            dec_layers.append(Inception(2*channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "        dec_layers.append(Inception(2*channel_hid, channel_hid//2, channel_in, incep_ker= incep_ker, groups=groups))\n",
    "\n",
    "        self.enc = nn.Sequential(*enc_layers)\n",
    "        self.dec = nn.Sequential(*dec_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B, T*C, H, W)\n",
    "\n",
    "        # encoder\n",
    "        skips = []\n",
    "        z = x\n",
    "        for i in range(self.N_T):\n",
    "            z = self.enc[i](z)\n",
    "            if i < self.N_T - 1:\n",
    "                skips.append(z)\n",
    "\n",
    "        # decoder\n",
    "        z = self.dec[0](z)\n",
    "        for i in range(1, self.N_T):\n",
    "            z = self.dec[i](torch.cat([z, skips[-i]], dim=1))\n",
    "\n",
    "        y = z.reshape(B, T, C, H, W)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SimVP(nn.Module):\n",
    "    def __init__(self, shape_in, hid_S=16, hid_T=256, N_S=4, N_T=8, incep_ker=[3,5,7,11], groups=8):\n",
    "        super(SimVP, self).__init__()\n",
    "        T, C, H, W = shape_in\n",
    "        self.enc = Encoder(C, hid_S, N_S)\n",
    "        self.hid = Mid_Xnet(T*hid_S, hid_T, N_T, incep_ker, groups)\n",
    "        self.dec = Decoder(hid_S, C, N_S)\n",
    "\n",
    "\n",
    "    def forward(self, x_raw):\n",
    "        B, T, C, H, W = x_raw.shape\n",
    "        x = x_raw.view(B*T, C, H, W)\n",
    "\n",
    "        embed, skip = self.enc(x)\n",
    "        _, C_, H_, W_ = embed.shape\n",
    "\n",
    "        z = embed.view(B, T, C_, H_, W_)\n",
    "        hid = self.hid(z)\n",
    "        hid = hid.reshape(B*T, C_, H_, W_)\n",
    "\n",
    "        Y = self.dec(hid, skip)\n",
    "        Y = Y.reshape(B, T, C, H, W)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pl Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberSSIMLoss2D(nn.Module):\n",
    "    def __init__(self, alpha=0.7, delta=0.05, window_size=5, temporal_weight=0.1):\n",
    "        super().__init__()\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # ALPHA (Controls balance between Huber Loss and SSIM)\n",
    "        # More alpha  → Model focuses on voxel accuracy (Huber loss)\n",
    "        # Less alpha  → Model prioritizes structural similarity (SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - 0.6 - 0.9 → Noisy data (CT/MRI with artifacts) → More Huber\n",
    "        # - 0.4 - 0.7 → Sharp structures (CT/MRI edges)  → Balance of both\n",
    "        # - 0.3 - 0.5 → Blurry predictions → More SSIM for finer details\n",
    "        # Default: alpha = 0.7 (Strong Huber, some SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.alpha = alpha  # More alpha = More reliance on Huber\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA (Threshold where Huber Loss switches from MSE-like to MAE-like behavior)\n",
    "        # - Higher delta → More sensitive to small errors (acts like MSE)\n",
    "        # - Lower delta  → More resistant to outliers (acts like MAE)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.05  → High-noise data (CT/MRI with artifacts) → More robust to outliers\n",
    "        # - < 0.05  → Low-noise data (Well-normalized, synthetic) → More sensitive to details\n",
    "        # - 0.02 - 0.05  → If predictions are too blurry\n",
    "        # Default: delta = 0.05 (or dynamically adjusted per epoch)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta = delta  # This will be dynamically updated every epoch\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # TEMPORAL WEIGHT (Penalizes abrupt voxel intensity changes between frames)\n",
    "        # - Higher value → Forces smoother transitions\n",
    "        # - Lower value  → Allows more flexibility in voxel changes\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.2  → Strong penalty for sudden intensity jumps (Flickering reduction)\n",
    "        # - 0.05 - 0.1  → Best for smoothly changing sequences (MRI, Weather, Fluids)\n",
    "        # - < 0.05  → Allows more dynamic changes (if model is too rigid)\n",
    "        # Default: temporal_weight = 0.1 (balanced smoothness)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.temporal_weight = temporal_weight  # More weight = Stronger smoothness enforcement\n",
    "\n",
    "        #window_size means padding\n",
    "        self.ssim_module = kornia.losses.SSIMLoss(window_size=window_size, reduction=\"mean\")\n",
    "\n",
    "    def temporal_smoothness_loss(self, y_pred):\n",
    "        #Penalizes sudden changes over time by computing L1 loss between consecutive frames.\n",
    "        #So the object should remain stationary over time\n",
    "        return torch.mean(torch.abs(y_pred[:, 1:] - y_pred[:, :-1]))  # Difference between t and t+1\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        B, T, C, H, W = y_pred.shape\n",
    "\n",
    "        # Swap time and channel dimensions: [B, C*T, H, W]\n",
    "        y_pred_restructured = y_pred.permute(0, 2, 1, 3, 4).reshape(B, C * T, H, W)\n",
    "        y_true_restructured = y_true.permute(0, 2, 1, 3, 4).reshape(B, C * T, H, W)\n",
    "\n",
    "        # Computes the SSIM treating time as another spatial dimension\n",
    "        ssim_loss = self.ssim_module(y_pred_restructured, y_true_restructured)\n",
    "\n",
    "        # Computes the Huber loss with the adjusted delta\n",
    "        huber_loss = F.huber_loss(y_pred, y_true, delta=self.delta, reduction=\"mean\")\n",
    "\n",
    "        # Compute the Temporal Smoothness Loss\n",
    "        temporal_loss = self.temporal_smoothness_loss(y_pred)\n",
    "\n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * huber_loss + (1 - self.alpha) * ssim_loss + self.temporal_weight * temporal_loss\n",
    "        return total_loss, huber_loss, ssim_loss, temporal_loss\n",
    "\n",
    "class HuberSSIMLoss3D(nn.Module):\n",
    "    def __init__(self, alpha=0.7, delta=0.05, window_size=5, temporal_weight=0.1):\n",
    "        super().__init__()\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # ALPHA (Controls balance between Huber Loss and SSIM)\n",
    "        # More alpha  → Model focuses on voxel accuracy (Huber loss)\n",
    "        # Less alpha  → Model prioritizes structural similarity (SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - 0.6 - 0.9 → Noisy data (CT/MRI with artifacts) → More Huber\n",
    "        # - 0.4 - 0.7 → Sharp structures (CT/MRI edges)  → Balance of both\n",
    "        # - 0.3 - 0.5 → Blurry predictions → More SSIM for finer details\n",
    "        # Default: alpha = 0.7 (Strong Huber, some SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.alpha = alpha  # More alpha = More reliance on Huber\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA (Threshold where Huber Loss switches from MSE-like to MAE-like behavior)\n",
    "        # - Higher delta → More sensitive to small errors (acts like MSE)\n",
    "        # - Lower delta  → More resistant to outliers (acts like MAE)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.05  → High-noise data (CT/MRI with artifacts) → More robust to outliers\n",
    "        # - < 0.05  → Low-noise data (Well-normalized, synthetic) → More sensitive to details\n",
    "        # - 0.02 - 0.05  → If predictions are too blurry\n",
    "        # Default: delta = 0.05 (or dynamically adjusted per epoch)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta = delta  # This will be dynamically updated every epoch\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # TEMPORAL WEIGHT (Penalizes abrupt voxel intensity changes between frames)\n",
    "        # - Higher value → Forces smoother transitions\n",
    "        # - Lower value  → Allows more flexibility in voxel changes\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.2  → Strong penalty for sudden intensity jumps (Flickering reduction)\n",
    "        # - 0.05 - 0.1  → Best for smoothly changing sequences (MRI, Weather, Fluids)\n",
    "        # - < 0.05  → Allows more dynamic changes (if model is too rigid)\n",
    "        # Default: temporal_weight = 0.1 (balanced smoothness)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.temporal_weight = temporal_weight  # More weight = Stronger smoothness enforcement\n",
    "\n",
    "        #window_size means padding\n",
    "        self.ssim_module = kornia.losses.SSIM3DLoss(window_size=window_size, reduction=\"mean\")\n",
    "\n",
    "    def temporal_smoothness_loss(self, y_pred):\n",
    "        #Penalizes sudden changes over time by computing L1 loss between consecutive frames.\n",
    "        #So the object should remain stationary over time\n",
    "        return torch.mean(torch.abs(y_pred[:, 1:] - y_pred[:, :-1]))  # Difference between t and t+1\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        B, T, C, D, H, W = y_pred.shape\n",
    "\n",
    "        # Swap time and channel dimensions: [B, C*T, D, H, W]\n",
    "        y_pred_restructured = y_pred.permute(0, 2, 1, 3, 4, 5).reshape(B, C * T, D, H, W)\n",
    "        y_true_restructured = y_true.permute(0, 2, 1, 3, 4, 5).reshape(B, C * T, D, H, W)\n",
    "\n",
    "        # Computes the SSIM treating time as another spatial dimension\n",
    "        ssim_loss = self.ssim_module(y_pred_restructured, y_true_restructured)\n",
    "\n",
    "        # Computes the Huber loss with the adjusted delta\n",
    "        huber_loss = F.huber_loss(y_pred, y_true, delta=self.delta, reduction=\"mean\")\n",
    "\n",
    "        # Compute the Temporal Smoothness Loss\n",
    "        temporal_loss = self.temporal_smoothness_loss(y_pred)\n",
    "\n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * huber_loss + (1 - self.alpha) * ssim_loss + self.temporal_weight * temporal_loss\n",
    "        return total_loss, huber_loss, ssim_loss, temporal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pl_Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        passed_model: nn.Module,\n",
    "        config: Dict[str, Any],\n",
    "    ):\n",
    "        super(Pl_Model, self).__init__()\n",
    "        self.passed_model = passed_model\n",
    "        self.config = config\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA FACTOR (Scales how much delta is updated per epoch)\n",
    "        # - Higher delta_factor → More aggressive updates to delta\n",
    "        # - Lower delta_factor  → Smoother, slower changes to delta\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 1.5  → If delta is too unstable (jumps too much)\n",
    "        # - 1.2 - 1.5  → Best for gradual adaptation (Default)\n",
    "        # - < 1.2  → If delta changes too slowly (use for very stable datasets)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Default: delta_factor = 1.2 (Balanced adaptation)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta_factor = 1.2  # More factor = Faster delta adjustments\n",
    "        self.delta = 0.05\n",
    "        self.previous_delta = self.delta\n",
    "        \n",
    "        #speicher alle parameter ab\n",
    "        self.save_hyperparameters(ignore=[\"passed_model\"])\n",
    "\n",
    "        # Setup training components\n",
    "        self.mse_criterion = nn.MSELoss()\n",
    "        self.psnr_criterion = PeakSignalNoiseRatio()\n",
    "        self.huberssim2d_criterion = HuberSSIMLoss2D()\n",
    "        self.huberssim3d_criterion = HuberSSIMLoss3D()\n",
    "        self.huber_criterion = nn.HuberLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.passed_model(x)\n",
    "        #Tanh has a larger gradient range, reducing saturation issues compared to sigmoid.\n",
    "        #Allows more stable gradient flow for deep networks.\n",
    "        x = 0.5*(F.tanh(x)+1)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Sets the Optimizer for the Model\"\"\"\n",
    "        optimizer = optim.AdamW(\n",
    "            self.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "        )\n",
    "        return [optimizer]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        \"\"\"Calculates the loss for a batch in different modes (training, validation, testing)\"\"\"\n",
    "        inputs, targets = batch\n",
    "\n",
    "        #forward pass\n",
    "        mse_loss = 0.0\n",
    "        huber_loss = 0.0\n",
    "        rmse_loss = 0.0\n",
    "        ssim_loss = 0.0\n",
    "        huberssim_loss = 0.0\n",
    "        temporal_loss = 0.0\n",
    "        psnr_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        #mse_loss_table = []\n",
    "        for t in range(0, self.config[\"pred_frames\"], self.config[\"pred_n_frames_per_step\"]):\n",
    "            if self.config[\"pred_frames\"]-t<self.config[\"pred_n_frames_per_step\"]:\n",
    "                frames_this_step = self.config[\"pred_frames\"]-t\n",
    "            else:\n",
    "                frames_this_step = self.config[\"pred_n_frames_per_step\"]\n",
    "            outputs = self.forward(inputs)\n",
    "            #print(f\"{t}:{t+frames_this_step}\")\n",
    "            #get only the first predicted frame\n",
    "            outputs = outputs[:, :frames_this_step, :, :]\n",
    "            \n",
    "            #calcualte losses\n",
    "            mse_loss_ = self.mse_criterion(outputs, targets[:, t:t+frames_this_step, :, :])\n",
    "            rmse_loss_ = torch.sqrt(self.mse_criterion(outputs, targets[:, t:t+frames_this_step, :, :]))\n",
    "            psnr_loss_ = self.psnr_criterion(outputs, targets[:, t:t+frames_this_step, :, :])\n",
    "            huberssim_loss_, huber_loss_, ssim_loss_, temporal_loss_ = self.huberssim2d_criterion(outputs, targets[:, t:t+frames_this_step, :, :])\n",
    "            total_loss_ = huberssim_loss_ #self.huber_criterion(outputs, targets[:, t:t+frames_this_step, :, :])  \n",
    "\n",
    "            #mse_loss_table.append([t, mse_loss_.item()])\n",
    "            \n",
    "            mse_loss += mse_loss_\n",
    "            huber_loss += huber_loss_\n",
    "            rmse_loss += rmse_loss_\n",
    "            ssim_loss += ssim_loss_\n",
    "            huberssim_loss += huberssim_loss_\n",
    "            temporal_loss += temporal_loss_\n",
    "            psnr_loss += psnr_loss_\n",
    "            total_loss += total_loss_\n",
    "            \n",
    "            inputs = torch.cat([inputs[:, self.config[\"pred_n_frames_per_step\"]:, :, :], outputs], dim=1)\n",
    "\n",
    "        #logging\n",
    "        self.log(f\"{mode}_mse_loss\", mse_loss)\n",
    "        self.log(f\"{mode}_huber_loss\", huber_loss)\n",
    "        self.log(f\"{mode}_rmse_loss\", rmse_loss)\n",
    "        self.log(f\"{mode}_ssim_loss\", ssim_loss)\n",
    "        self.log(f\"{mode}_huberssim_loss\", huberssim_loss)\n",
    "        self.log(f\"{mode}_temporal_loss\", temporal_loss)\n",
    "        self.log(f\"{mode}_psnr_loss\", psnr_loss)\n",
    "        self.log(f\"{mode}_total_loss\", total_loss, prog_bar=True)\n",
    "\n",
    "        #table logging\n",
    "        #wandb.log({\"Loss per Timestep\": wandb.plot.line(wandb.Table(data=mse_loss_table, columns=[\"Timestep\", \"Loss\"]), \"Timestep\", \"Loss\", title=\"Loss per Timestep\")})\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx):\n",
    "        if dataloader_idx==1:\n",
    "            inputs, targets = batch\n",
    "            # iterate over depth\n",
    "            outputs = []\n",
    "            for d in range(inputs.shape[3]):\n",
    "                inputs_ = inputs[:, :, :, d, :, :]\n",
    "                #iterate over time\n",
    "                outputs_d = []\n",
    "                for t in range(0, inputs.shape[1], self.config[\"pred_n_frames_per_step\"]):\n",
    "                    if inputs.shape[1]-t<self.config[\"pred_n_frames_per_step\"]:\n",
    "                        frames_this_step = inputs.shape[1]-t\n",
    "                    else:\n",
    "                        frames_this_step = self.config[\"pred_n_frames_per_step\"]\n",
    "                    \n",
    "                    outputs_t = self.forward(inputs_)\n",
    "            \n",
    "                    #get only the first predicted frame\n",
    "                    outputs_t = outputs_t[:, :frames_this_step, :, :]\n",
    "            \n",
    "                    #add to depth lst\n",
    "                    outputs_d.append(outputs_t)\n",
    "            \n",
    "                    inputs_ = torch.cat([inputs_[:, self.config[\"pred_n_frames_per_step\"]:, :, :], outputs_t], dim=1)\n",
    "            \n",
    "                #concat time and add to overall lst\n",
    "                outputs_d = torch.concat(outputs_d, dim=1)\n",
    "                outputs.append(outputs_d)\n",
    "            #stack over depth\n",
    "            outputs = torch.stack(outputs, dim=3)\n",
    "\n",
    "            #calculate losses\n",
    "            mse_loss = self.mse_criterion(outputs, targets)\n",
    "            rmse_loss = torch.sqrt(self.mse_criterion(outputs, targets))\n",
    "            huberssim_loss, huber_loss, ssim_loss, temporal_loss = self.huberssim3d_criterion(outputs, targets)\n",
    "            psnr_loss = self.psnr_criterion(outputs, targets)\n",
    "            total_loss = huberssim_loss#self.huber_criterion(outputs, targets)\n",
    "\n",
    "            #logging\n",
    "            self.log(f\"overall_val_mse_loss\", mse_loss)\n",
    "            self.log(f\"overall_val_huber_loss\", huber_loss)\n",
    "            self.log(f\"overall_val_rmse_loss\", rmse_loss)\n",
    "            self.log(f\"overall_val_ssim_loss\", ssim_loss)\n",
    "            self.log(f\"overall_val_huberssim_loss\", huberssim_loss)\n",
    "            self.log(f\"overall_val_temporal_loss\", temporal_loss)\n",
    "            self.log(f\"overall_val_psnr_loss\", psnr_loss)\n",
    "            self.log(f\"overall_val_total_loss\", total_loss)\n",
    "                    \n",
    "        else:\n",
    "            _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        #adjust delta\n",
    "        val_loader = self.trainer.datamodule.val_dataloader()[0]\n",
    "        all_errors = []\n",
    "    \n",
    "        with torch.no_grad():  \n",
    "            for batch in val_loader:\n",
    "                x, y = batch\n",
    "                y_pred = self(x.to(self.device))\n",
    "                error = torch.abs(y.to(self.device) - y_pred)\n",
    "                all_errors.append(error.view(-1))\n",
    "    \n",
    "        all_errors = torch.cat(all_errors)\n",
    "        new_delta = self.delta_factor * torch.std(all_errors).item()\n",
    "\n",
    "        #Blend previous and new delta for smoother updates\n",
    "        #is capped between 0.02 and 0.35 so that is the data is too noisy huber does not just become mse\n",
    "        new_delta = min(0.5, max(0.02, 0.8 * self.previous_delta + 0.2 * new_delta))\n",
    "        self.previous_delta = new_delta\n",
    "\n",
    "        #update\n",
    "        self.huberssim2d_criterion.delta = new_delta\n",
    "        self.huberssim3d_criterion.delta = new_delta\n",
    "    \n",
    "        #logging\n",
    "        self.log(\"delta\", new_delta)\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def check_losses(self, loader, mode, use_wandb=False):\n",
    "        mse_loss = 0.0\n",
    "        huber_loss = 0.0\n",
    "        rmse_loss = 0.0\n",
    "        ssim_loss = 0.0\n",
    "        huberssim_loss = 0.0\n",
    "        temporal_loss = 0.0\n",
    "        psnr_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in loader:\n",
    "            for t in range(self.config[\"pred_frames\"]):\n",
    "                mse_loss_ = self.mse_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                huberssim_loss_, huber_loss_, ssim_loss_, temporal_loss_ = self.huberssim2d_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                rmse_loss_ = torch.sqrt(self.mse_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1)))\n",
    "                psnr_loss_ = self.psnr_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                total_loss_ = huberssim_loss_   \n",
    "                \n",
    "                mse_loss += mse_loss_.item()\n",
    "                huber_loss += huber_loss_.item()\n",
    "                rmse_loss += rmse_loss_.item()\n",
    "                ssim_loss += ssim_loss_.item()\n",
    "                huberssim_loss += huberssim_loss_.item()\n",
    "                temporal_loss += temporal_loss_.item()\n",
    "                psnr_loss += psnr_loss_.item()\n",
    "                total_loss += total_loss_.item()\n",
    "                \n",
    "        mse_loss = mse_loss / len(loader)\n",
    "        huber_loss = huber_loss / len(loader)\n",
    "        rmse_loss = rmse_loss / len(loader)\n",
    "        ssim_loss = ssim_loss / len(loader)\n",
    "        huberssim_loss = huberssim_loss / len(loader)\n",
    "        temporal_loss = temporal_loss / len(loader)\n",
    "        psnr_loss = psnr_loss / len(loader)\n",
    "        total_loss = total_loss / len(loader)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({f\"Checked_{mode}_mse_loss\": mse_loss})\n",
    "            wandb.log({f\"Checked_{mode}_huber_loss\": huber_loss})\n",
    "            wandb.log({f\"Checked_{mode}_rmse_loss\": rmse_loss})\n",
    "            wandb.log({f\"Checked_{mode}_ssim_loss\": ssim_loss})\n",
    "            wandb.log({f\"Checked_{mode}_huberssim_loss\": huberssim_loss})\n",
    "            wandb.log({f\"Checked_{mode}_temporal_loss\": temporal_loss})\n",
    "            wandb.log({f\"Checked_{mode}_psnr_loss\": psnr_loss})\n",
    "            wandb.log({f\"Checked_{mode}_total_loss\": total_loss})\n",
    "        \n",
    "        return mse_loss, huber_loss, ssim_loss, huberssim_loss, temporal_loss, rmse_loss, psnr_loss, total_loss\n",
    "        \n",
    "    def log_predictions(self):\n",
    "        \"\"\"Log example predictions to wandb\"\"\"\n",
    "        #needs to be added to other method\n",
    "        if epoch % self.config['viz_interval'] == 0:\n",
    "                self.log_predictions()\n",
    "        #but this whole method needs to be rewritten\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of validation data\n",
    "            data, target = next(iter(self.val_loader))\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            \n",
    "            # Generate predictions\n",
    "            output = self.model(data)\n",
    "            \n",
    "            # Log images\n",
    "            wandb.log({\n",
    "                \"predictions\": wandb.Image(output[0, 0].cpu()),\n",
    "                \"targets\": wandb.Image(target[0, 0].cpu()),\n",
    "                \"input_sequence\": [wandb.Image(data[0, i].cpu()) for i in range(data.shape[1])]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"root\": \"../NormalizedQualityFiltered\",\n",
    "    'batch_size': 29,\n",
    "    'learning_rate': 0.0001,\n",
    "    \"num_workers\": 10,#0, wenn die gpu nicht benutzt wird\n",
    "    \"pin_memory\": True if torch.cuda.is_available() else False,#False, wenn die gpu nicht benutzt wird\n",
    "    \"drop_last\": False,\n",
    "    'epochs': 40,\n",
    "    #'log_interval': 20,\n",
    "    #'viz_interval': 1,\n",
    "    'run_name': '2D-SimVP',\n",
    "    'input_frames': 9,\n",
    "    \"pred_frames\": 9,\n",
    "    \"pred_n_frames_per_step\": 9,\n",
    "    'base_filters': 32,\n",
    "    \"train_split\": 0.7,\n",
    "    \"val_split\": 0.15,\n",
    "    \"test_split\": 0.15,\n",
    "}\n",
    "config[\"run_name\"] += f\"_{config['pred_frames']}\"\n",
    "if config[\"pred_frames\"] == config[\"pred_n_frames_per_step\"]:\n",
    "    config[\"run_name\"] += \"_NAR\"\n",
    "elif config[\"pred_n_frames_per_step\"] == 1:\n",
    "    config[\"run_name\"] += \"_FAR\"\n",
    "else:\n",
    "    config[\"run_name\"] += f\"_PAR_{config['pred_n_frames_per_step']}\"\n",
    "\n",
    "# Get data loaders\n",
    "'''train_loader, val_loader, test_loader, val_loader_3d, test_loader_3d, train_dataset, val_dataset, test_dataset, val_dataset_3d, test_dataset_3d = get_data_loaders(\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=config[\"pin_memory\"],\n",
    "    drop_last=config[\"drop_last\"],\n",
    "    sequence_length=config[\"input_frames\"], \n",
    "    prediction_length=config[\"pred_frames\"],\n",
    ")'''\n",
    "dm = VolumeDataModule(\n",
    "    root=config[\"root\"],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=config[\"pin_memory\"],\n",
    "    drop_last=config[\"drop_last\"],\n",
    "    sequence_length=config[\"input_frames\"],\n",
    "    prediction_length=config[\"pred_frames\"],\n",
    "    train_split=config[\"train_split\"],\n",
    "    val_split=config[\"val_split\"],\n",
    "    test_split=config[\"test_split\"],\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(entity=\"ChadCTP\", project=\"perfusion-ct-prediction\", name=config[\"run_name\"])\n",
    "\n",
    "# Initialize model for tuning\n",
    "model = SimVP(\n",
    "    shape_in=[config[\"input_frames\"], 1, 256, 256],\n",
    "    # hid_S=128, \n",
    "    # hid_T=256, \n",
    "    # N_S=8, \n",
    "    # N_T=8, \n",
    "    # incep_ker=[3,5,7,11], \n",
    "    # groups=8,\n",
    ")\n",
    "\n",
    "# Initialize pl_model for tuning\n",
    "pl_model = Pl_Model(\n",
    "    passed_model=model,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_total_loss/dataloader_idx_0\",  \n",
    "    mode=\"min\",  \n",
    "    save_top_k=1,  \n",
    "    filename=\"best-checkpoint\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer for tuning\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    accelerator=\"gpu\",\n",
    "    devices= [1] if torch.cuda.is_available() else None,\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    callbacks=[RichProgressBar(), checkpoint_callback],\n",
    "    check_val_every_n_epoch=5,\n",
    ")\n",
    "\n",
    "#wandb_logger.watch(pl_model, log_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250316_175225-oqelujc5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/oqelujc5' target=\"_blank\">2D-SimVP_9_NAR</a></strong> to <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction' target=\"_blank\">https://wandb.ai/ChadCTP/perfusion-ct-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/oqelujc5' target=\"_blank\">https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/oqelujc5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                 </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ passed_model          │ SimVP                │ 13.8 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ mse_criterion         │ MSELoss              │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ psnr_criterion        │ PeakSignalNoiseRatio │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ huberssim2d_criterion │ HuberSSIMLoss2D      │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ huberssim3d_criterion │ HuberSSIMLoss3D      │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ huber_criterion       │ HuberLoss            │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ passed_model          │ SimVP                │ 13.8 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ mse_criterion         │ MSELoss              │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ psnr_criterion        │ PeakSignalNoiseRatio │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ huberssim2d_criterion │ HuberSSIMLoss2D      │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ huberssim3d_criterion │ HuberSSIMLoss3D      │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ huber_criterion       │ HuberLoss            │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 13.8 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 13.8 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 55                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 360                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 13.8 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 13.8 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 55                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 360                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bdfcf875c34b0f96cb2359f1327c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches \n",
       "(46) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\n",
       "you want to see logs for the training epoch.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches \n",
       "(46) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\n",
       "you want to see logs for the training epoch.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 230: 'val_total_loss/dataloader_idx_0' reached 0.09117 (best 0.09117), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 9, global step 460: 'val_total_loss/dataloader_idx_0' reached 0.05997 (best 0.05997), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 14, global step 690: 'val_total_loss/dataloader_idx_0' reached 0.02429 (best 0.02429), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 19, global step 920: 'val_total_loss/dataloader_idx_0' reached 0.01268 (best 0.01268), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 24, global step 1150: 'val_total_loss/dataloader_idx_0' reached 0.00959 (best 0.00959), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 29, global step 1380: 'val_total_loss/dataloader_idx_0' reached 0.00810 (best 0.00810), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 34, global step 1610: 'val_total_loss/dataloader_idx_0' reached 0.00724 (best 0.00724), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 39, global step 1840: 'val_total_loss/dataloader_idx_0' reached 0.00668 (best 0.00668), saving model to './perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=pl_model,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f90d659169c4e4c8e098898fb0fa4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_huber_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  4.0836679545463994e-05   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_huberssim_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.006307575851678848    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_mse_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0002555459795985371   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_psnr_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     36.12557601928711     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_rmse_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.01579868607223034    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_ssim_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.020451875403523445    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_temporal_loss     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0014342714566737413   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_total_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.006307575851678848    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_huber_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 4.0836679545463994e-05  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_huberssim_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.006307575851678848   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_mse_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0002555459795985371  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_psnr_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    36.12557601928711    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_rmse_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.01579868607223034   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_ssim_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.020451875403523445   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_temporal_loss    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0014342714566737413  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_total_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.006307575851678848   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check and log the losses \"to beat\"\n",
    "dm.setup()\n",
    "pl_model.check_losses(dm.train_dataloader(), mode=\"train\", use_wandb=True)\n",
    "pl_model.check_losses(dm.val_dataloader()[0], mode=\"val\", use_wandb=True)\n",
    "pl_model.check_losses(dm.test_dataloader(), mode=\"test\", use_wandb=True)\n",
    "\n",
    "#val_results = trainer.validate(pl_model, datamodule=dm)\n",
    "test_results = trainer.test(pl_model, datamodule=dm)\n",
    "\n",
    "save_load_path = f\"../ModelWeights/{config['run_name']}.ckpt\"\n",
    "trainer.save_checkpoint(save_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Pl_Model:\n\tUnexpected key(s) in state_dict: \"passed_model.enc.enc.4.conv.conv.weight\", \"passed_model.enc.enc.4.conv.conv.bias\", \"passed_model.enc.enc.4.conv.norm.weight\", \"passed_model.enc.enc.4.conv.norm.bias\", \"passed_model.enc.enc.5.conv.conv.weight\", \"passed_model.enc.enc.5.conv.conv.bias\", \"passed_model.enc.enc.5.conv.norm.weight\", \"passed_model.enc.enc.5.conv.norm.bias\", \"passed_model.enc.enc.6.conv.conv.weight\", \"passed_model.enc.enc.6.conv.conv.bias\", \"passed_model.enc.enc.6.conv.norm.weight\", \"passed_model.enc.enc.6.conv.norm.bias\", \"passed_model.enc.enc.7.conv.conv.weight\", \"passed_model.enc.enc.7.conv.conv.bias\", \"passed_model.enc.enc.7.conv.norm.weight\", \"passed_model.enc.enc.7.conv.norm.bias\", \"passed_model.dec.dec.4.conv.conv.weight\", \"passed_model.dec.dec.4.conv.conv.bias\", \"passed_model.dec.dec.4.conv.norm.weight\", \"passed_model.dec.dec.4.conv.norm.bias\", \"passed_model.dec.dec.5.conv.conv.weight\", \"passed_model.dec.dec.5.conv.conv.bias\", \"passed_model.dec.dec.5.conv.norm.weight\", \"passed_model.dec.dec.5.conv.norm.bias\", \"passed_model.dec.dec.6.conv.conv.weight\", \"passed_model.dec.dec.6.conv.conv.bias\", \"passed_model.dec.dec.6.conv.norm.weight\", \"passed_model.dec.dec.6.conv.norm.bias\", \"passed_model.dec.dec.7.conv.conv.weight\", \"passed_model.dec.dec.7.conv.conv.bias\", \"passed_model.dec.dec.7.conv.norm.weight\", \"passed_model.dec.dec.7.conv.norm.bias\". \n\tsize mismatch for passed_model.enc.enc.0.conv.conv.weight: copying a param with shape torch.Size([64, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.0.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.0.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.0.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.1.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.1.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.1.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.1.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.2.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.2.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.2.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.2.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.3.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.3.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.3.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.3.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.hid.enc.0.conv1.weight: copying a param with shape torch.Size([128, 576, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 144, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.0.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.0.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.0.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.0.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.1.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.1.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.1.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.1.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.1.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.2.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.2.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.2.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.2.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.2.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.3.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.3.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.3.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.3.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.3.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.4.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.4.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.4.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.4.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.4.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.5.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.5.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.5.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.5.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.5.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.6.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.6.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.6.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.6.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.6.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.7.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.7.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.7.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.7.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.7.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.0.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.0.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.0.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.0.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.1.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.1.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.1.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.1.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.2.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.2.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.2.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.2.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.2.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.3.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.3.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.3.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.3.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.3.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.4.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.4.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.4.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.4.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.4.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.5.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.5.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.5.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.5.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.5.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.6.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.6.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.6.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.6.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.6.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.7.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.conv.weight: copying a param with shape torch.Size([576, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([144, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.conv.weight: copying a param with shape torch.Size([576, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([144, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.conv.weight: copying a param with shape torch.Size([576, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([144, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.conv.weight: copying a param with shape torch.Size([576, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([144, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.dec.dec.0.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.0.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.0.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.0.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.1.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.1.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.1.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.1.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.2.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.2.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.2.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.2.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.3.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 32, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.3.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.3.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.3.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.readout.weight: copying a param with shape torch.Size([1, 64, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 16, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#testing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pl_model = \u001b[43mPl_Model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./perfusion-ct-prediction/bd1pw79f/checkpoints/best-checkpoint.ckpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m dm.setup()\n\u001b[32m      8\u001b[39m test_dataloader = dm.test_dataloader()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    122\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/module.py:1581\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[39m\n\u001b[32m   1492\u001b[39m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_checkpoint\u001b[39m(\n\u001b[32m   1494\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1499\u001b[39m     **kwargs: Any,\n\u001b[32m   1500\u001b[39m ) -> Self:\n\u001b[32m   1501\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[33;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1503\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1579\u001b[39m \n\u001b[32m   1580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1581\u001b[39m     loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1582\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/saving.py:91\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, **kwargs)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl.LightningModule):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     model = \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     state_dict = checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/saving.py:187\u001b[39m, in \u001b[36m_load_state\u001b[39m\u001b[34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[39m\n\u001b[32m    184\u001b[39m     obj.on_load_checkpoint(checkpoint)\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m keys = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_dict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m keys.missing_keys:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2584\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2576\u001b[39m         error_msgs.insert(\n\u001b[32m   2577\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2578\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2579\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2580\u001b[39m             ),\n\u001b[32m   2581\u001b[39m         )\n\u001b[32m   2583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2586\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2587\u001b[39m         )\n\u001b[32m   2588\u001b[39m     )\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for Pl_Model:\n\tUnexpected key(s) in state_dict: \"passed_model.enc.enc.4.conv.conv.weight\", \"passed_model.enc.enc.4.conv.conv.bias\", \"passed_model.enc.enc.4.conv.norm.weight\", \"passed_model.enc.enc.4.conv.norm.bias\", \"passed_model.enc.enc.5.conv.conv.weight\", \"passed_model.enc.enc.5.conv.conv.bias\", \"passed_model.enc.enc.5.conv.norm.weight\", \"passed_model.enc.enc.5.conv.norm.bias\", \"passed_model.enc.enc.6.conv.conv.weight\", \"passed_model.enc.enc.6.conv.conv.bias\", \"passed_model.enc.enc.6.conv.norm.weight\", \"passed_model.enc.enc.6.conv.norm.bias\", \"passed_model.enc.enc.7.conv.conv.weight\", \"passed_model.enc.enc.7.conv.conv.bias\", \"passed_model.enc.enc.7.conv.norm.weight\", \"passed_model.enc.enc.7.conv.norm.bias\", \"passed_model.dec.dec.4.conv.conv.weight\", \"passed_model.dec.dec.4.conv.conv.bias\", \"passed_model.dec.dec.4.conv.norm.weight\", \"passed_model.dec.dec.4.conv.norm.bias\", \"passed_model.dec.dec.5.conv.conv.weight\", \"passed_model.dec.dec.5.conv.conv.bias\", \"passed_model.dec.dec.5.conv.norm.weight\", \"passed_model.dec.dec.5.conv.norm.bias\", \"passed_model.dec.dec.6.conv.conv.weight\", \"passed_model.dec.dec.6.conv.conv.bias\", \"passed_model.dec.dec.6.conv.norm.weight\", \"passed_model.dec.dec.6.conv.norm.bias\", \"passed_model.dec.dec.7.conv.conv.weight\", \"passed_model.dec.dec.7.conv.conv.bias\", \"passed_model.dec.dec.7.conv.norm.weight\", \"passed_model.dec.dec.7.conv.norm.bias\". \n\tsize mismatch for passed_model.enc.enc.0.conv.conv.weight: copying a param with shape torch.Size([64, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.0.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.0.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.0.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.1.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.1.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.1.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.1.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.2.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.2.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.2.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.2.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.3.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.enc.enc.3.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.3.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.enc.enc.3.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.hid.enc.0.conv1.weight: copying a param with shape torch.Size([128, 576, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 144, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.0.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.0.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.0.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.0.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.1.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.1.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.1.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.1.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.1.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.2.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.2.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.2.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.2.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.2.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.3.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.3.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.3.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.3.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.3.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.4.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.4.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.4.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.4.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.4.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.5.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.5.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.5.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.5.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.5.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.6.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.6.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.6.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.6.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.6.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.enc.7.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.enc.7.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.enc.7.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.enc.7.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.enc.7.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.0.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.0.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.0.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.0.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.1.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.1.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.1.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.1.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.2.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.2.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.2.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.2.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.2.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.3.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.3.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.3.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.3.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.3.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.4.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.4.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.4.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.4.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.4.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.5.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.5.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.5.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.5.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.5.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.6.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.6.layers.0.conv.weight: copying a param with shape torch.Size([256, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.6.layers.1.conv.weight: copying a param with shape torch.Size([256, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([256, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.6.layers.2.conv.weight: copying a param with shape torch.Size([256, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([256, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.6.layers.3.conv.weight: copying a param with shape torch.Size([256, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([256, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.7.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.conv.weight: copying a param with shape torch.Size([576, 16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([144, 16, 3, 3]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.0.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.conv.weight: copying a param with shape torch.Size([576, 16, 5, 5, 5]) from checkpoint, the shape in current model is torch.Size([144, 16, 5, 5]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.1.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.conv.weight: copying a param with shape torch.Size([576, 16, 7, 7, 7]) from checkpoint, the shape in current model is torch.Size([144, 16, 7, 7]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.2.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.conv.weight: copying a param with shape torch.Size([576, 16, 11, 11, 11]) from checkpoint, the shape in current model is torch.Size([144, 16, 11, 11]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.conv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.hid.dec.7.layers.3.norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([144]).\n\tsize mismatch for passed_model.dec.dec.0.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.0.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.0.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.0.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.1.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.1.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.1.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.1.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.2.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.2.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.2.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.2.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.3.conv.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 32, 3, 3]).\n\tsize mismatch for passed_model.dec.dec.3.conv.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.3.conv.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.dec.3.conv.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for passed_model.dec.readout.weight: copying a param with shape torch.Size([1, 64, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 16, 1, 1])."
     ]
    }
   ],
   "source": [
    "#testing\n",
    "pl_model = Pl_Model.load_from_checkpoint(\n",
    "    \"./perfusion-ct-prediction/oqelujc5/checkpoints/best-checkpoint.ckpt\",\n",
    "    passed_model=model,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "test_dataloader = dm.test_dataloader()\n",
    "\n",
    "print(len(test_dataloader))\n",
    "pl_model.to(device)\n",
    "\"\"\"for inputs, targets in test_dataloader:\n",
    "    outputs = pl_model.forward(inputs.to(device))\n",
    "    outputs = outputs.detach().cpu()\n",
    "    outputs = torch.concat([inputs, outputs], dim=1).squeeze(0, 2).numpy()\n",
    "    targets = torch.concat([inputs, targets], dim=1).squeeze(0, 2).numpy()\n",
    "    print(outputs.shape)\n",
    "    print(targets.shape)\n",
    "    break\"\"\"\n",
    "# method 1\n",
    "#inputs, targets = next(iter(test_dataloader))\n",
    "\n",
    "# method 2\n",
    "# only for single shot\n",
    "vol = torch.tensor(np.load(\"../NormalizedQualityFiltered/MOL-061.npy\")).unsqueeze(1)\n",
    "inputs = vol[0:9]#.unsqueeze(0)\n",
    "targets = vol[9:]#.unsqueeze(0)\n",
    "print(inputs.shape)\n",
    "inputs_ = torch.concat([inputs.transpose(0, 2).transpose(1, 2), torch.zeros([config[\"batch_size\"]-vol.shape[2], config[\"input_frames\"], 1, vol.shape[-2], vol.shape[-1]])])\n",
    "outputs = pl_model(inputs_.to(device))\n",
    "outputs = outputs[:vol.shape[2]]\n",
    "print(outputs.shape)\n",
    "outputs = outputs.detach().cpu()\n",
    "outputs = torch.concat([inputs, outputs.transpose(0, 2).transpose(0, 1)], dim=0).squeeze(1).numpy()\n",
    "targets = torch.concat([inputs, targets], dim=1).squeeze(1).numpy()\n",
    "\n",
    "print(outputs.shape)\n",
    "print(targets.shape)\n",
    "#for i in range(inputs.shape[2]):\n",
    "#    print(inputs[:, :, i, :, :].shape)\n",
    "\n",
    "np.save(f\"outputs_{config[\"run_name\"]}.npy\", outputs)\n",
    "np.save(f\"targets_{config[\"run_name\"]}.npy\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def overall_loss(model, loader, device):\n",
    "    mse_loss = 0.0\n",
    "    huber_loss = 0.0\n",
    "    rmse_loss = 0.0\n",
    "    #ssim_loss = 0.0\n",
    "    psnr_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # iterate over depth\n",
    "        outputs = []\n",
    "        for d in range(inputs.shape[3]):\n",
    "            inputs_ = inputs[:, :, :, d, :, :]\n",
    "            #iterate over time\n",
    "            outputs_d = []\n",
    "            for t in range(0, model.config[\"pred_frames\"], model.config[\"pred_n_frames_per_step\"]):\n",
    "                if model.config[\"pred_frames\"]-t<model.config[\"pred_n_frames_per_step\"]:\n",
    "                    frames_this_step = model.config[\"pred_frames\"]-t\n",
    "                else:\n",
    "                    frames_this_step = model.config[\"pred_n_frames_per_step\"]\n",
    "                \n",
    "                outputs_t = model.forward(inputs_)\n",
    "        \n",
    "                #get only the first predicted frame\n",
    "                outputs_t = outputs_t[:, :frames_this_step, :, :]\n",
    "\n",
    "                #add to depth lst\n",
    "                outputs_d.append(outputs_t)\n",
    "        \n",
    "                inputs_ = torch.cat([inputs_[:, model.config[\"pred_n_frames_per_step\"]:, :, :], outputs_t], dim=1)\n",
    "            #concat time and add to overall lst\n",
    "            outputs_d = torch.concat(outputs_d, dim=1)\n",
    "            outputs.append(outputs_d)\n",
    "    \n",
    "        #stack over depth\n",
    "        outputs = torch.stack(outputs, dim=3)\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        #calculate losses\n",
    "        mse_loss += model.mse_criterion(outputs, targets).item()\n",
    "        huber_loss += model.huber_criterion(outputs, targets).item()\n",
    "        rmse_loss += torch.sqrt(model.mse_criterion(outputs, targets)).item()\n",
    "        #ssim_loss = model.ssim_criterion(outputs, targets).item()\n",
    "        psnr_loss += model.psnr_criterion(outputs, targets).item()\n",
    "        total_loss += huber_loss\n",
    "\n",
    "    mse_loss = mse_loss / len(loader)\n",
    "    huber_loss = huber_loss / len(loader)\n",
    "    rmse_loss = rmse_loss / len(loader)\n",
    "    #ssim_loss = ssim_loss / len(loader)\n",
    "    psnr_loss = psnr_loss / len(loader)\n",
    "    total_loss = total_loss / len(loader)\n",
    "\n",
    "    return outputs, mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss\n",
    "\n",
    "dm.setup()\n",
    "_, mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss = overall_loss(model=pl_model, loader=dm.test_dataloader_3d(), device=device)\n",
    "mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# idk tuning behaves really weird and different from fit regarding memory usage\n",
    "#tuning\n",
    "tuner = Tuner(trainer)\n",
    "tuner.scale_batch_size(pl_model, datamodule=dm, mode=\"binsearch\")\n",
    "\n",
    "#cleaning up\n",
    "del tuner\n",
    "del trainer\n",
    "del pl_model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Reinitialize model for tuning\n",
    "model = SimVP(\n",
    "    shape_in=[config[\"input_frames\"], 1, 256, 256]\n",
    ")\n",
    "\n",
    "# Reinitialize pl_model for training\n",
    "pl_model = Pl_Model(\n",
    "    passed_model=model,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Reinitialize trainer for training\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    accelerator=\"gpu\",\n",
    "    devices= [2] if torch.cuda.is_available() else None,\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    callbacks=[RichProgressBar()],\n",
    "    check_val_every_n_epoch=1,\n",
    "    #enable_checkpointing=False,\n",
    ")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
