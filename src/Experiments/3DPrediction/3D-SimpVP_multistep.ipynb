{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.8/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnbennewiz\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from ipywidgets import IntSlider, interact\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint\n",
    "from pytorch_lightning.tuner import Tuner\n",
    "\n",
    "import kornia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset3D(Dataset):\n",
    "    def __init__(self, data_paths, context_window=4, prediction_window=1, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.context_window = context_window\n",
    "        self.prediction_window = prediction_window\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        # For every path to a volume sequence in .npy\n",
    "        for data_path in self.data_paths:\n",
    "            volume_seq = np.load(data_path)\n",
    "            # Convert to tensor\n",
    "            volume_seq = torch.from_numpy(volume_seq)\n",
    "\n",
    "            for t in range(volume_seq.shape[0]-context_window-prediction_window+1):\n",
    "                self.samples.append([volume_seq[t:t+context_window].unsqueeze(1), volume_seq[t+context_window:t+context_window+prediction_window].unsqueeze(1)])\n",
    "                #input-shape: [T, C, D, H, W]\n",
    "                #target-shape: [T, C, D, H, W]\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, root, batch_size=4, sequence_length=4, prediction_length=1, num_workers=0, drop_last=False, pin_memory=False, train_split=0.8, val_split=0.1, test_split=0.1):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.pin_memory = pin_memory\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_paths = [os.path.join(self.root, path) for path in os.listdir(self.root)]\n",
    "        total_size = len(data_paths)\n",
    "    \n",
    "        # Normalize splits if they don’t sum to 1\n",
    "        split_sum = self.train_split + self.val_split + self.test_split\n",
    "        if split_sum != 1.0:\n",
    "            self.train_split /= split_sum\n",
    "            self.val_split /= split_sum\n",
    "            self.test_split /= split_sum\n",
    "            print(f\"Normalized splits to: train={self.train_split:.2f}, val={self.val_split:.2f}, test={self.test_split:.2f}\")\n",
    "    \n",
    "        # Compute dataset sizes\n",
    "        train_size = int(total_size * self.train_split)\n",
    "        val_size = int(total_size * self.val_split)\n",
    "        test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "    \n",
    "        # Error handling: Ensure valid split sizes\n",
    "        if train_size <= 0 or val_size <= 0 or test_size <= 0:\n",
    "            raise ValueError(f\"Invalid dataset splits: train={train_size}, val={val_size}, test={test_size}. Check your split values.\")\n",
    "    \n",
    "        # Perform random split\n",
    "        train_paths, val_paths, test_paths = random_split(data_paths, [train_size, val_size, test_size])\n",
    "    \n",
    "        self.train_dataset = Dataset3D(train_paths, self.sequence_length, self.prediction_length)\n",
    "        self.val_dataset = Dataset3D(val_paths, self.sequence_length, self.prediction_length)\n",
    "        self.test_dataset = Dataset3D(test_paths, self.sequence_length, self.prediction_length)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory),\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory)\n",
    "\n",
    "    def teardown(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after training...\")\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after testing...\")\n",
    "\n",
    "        if stage == \"validate\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after validation...\")\n",
    "\n",
    "        # Free memory by deleting large datasets\n",
    "        del self.train_dataset\n",
    "        del self.val_dataset\n",
    "        del self.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, transpose=False, act_norm=False):\n",
    "        super(BasicConv3d, self).__init__()\n",
    "        self.act_norm=act_norm\n",
    "        if not transpose:\n",
    "            self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.conv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,output_padding=stride //2 )\n",
    "        self.norm = nn.GroupNorm(2, out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.act(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvSC(nn.Module):\n",
    "    def __init__(self, C_in, C_out, stride, transpose=False, act_norm=True):\n",
    "        super(ConvSC, self).__init__()\n",
    "        if stride == 1:\n",
    "            transpose = False\n",
    "        self.conv = BasicConv3d(C_in, C_out, kernel_size=3, stride=stride,\n",
    "                                padding=1, transpose=transpose, act_norm=act_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GroupConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, act_norm=False):\n",
    "        super(GroupConv3d, self).__init__()\n",
    "        self.act_norm = act_norm\n",
    "        if in_channels % groups != 0:\n",
    "            groups = 1\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,groups=groups)\n",
    "        self.norm = nn.GroupNorm(groups,out_channels)\n",
    "        self.activate = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.activate(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, C_in, C_hid, C_out, incep_ker=[3,5,7,11], groups=8):        \n",
    "        super(Inception, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(C_in, C_hid, kernel_size=1, stride=1, padding=0)\n",
    "        layers = []\n",
    "        for ker in incep_ker:\n",
    "            layers.append(GroupConv3d(C_hid, C_out, kernel_size=ker, stride=1, padding=ker//2, groups=groups, act_norm=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y = 0\n",
    "        for layer in self.layers:\n",
    "            y += layer(x)\n",
    "        return y\n",
    "\n",
    "def stride_generator(N, reverse=False):\n",
    "    strides = [1, 2]*10\n",
    "    if reverse: return list(reversed(strides[:N]))\n",
    "    else: return strides[:N]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,C_in, C_hid, N_S):\n",
    "        super(Encoder,self).__init__()\n",
    "        strides = stride_generator(N_S)\n",
    "        self.enc = nn.Sequential(\n",
    "            ConvSC(C_in, C_hid, stride=strides[0]),\n",
    "            *[ConvSC(C_hid, C_hid, stride=s) for s in strides[1:]]\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):# B*4, 3, 128, 128\n",
    "        enc1 = self.enc[0](x)\n",
    "        latent = enc1\n",
    "        for i in range(1,len(self.enc)):\n",
    "            latent = self.enc[i](latent)\n",
    "        return latent,enc1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,C_hid, C_out, N_S):\n",
    "        super(Decoder,self).__init__()\n",
    "        strides = stride_generator(N_S, reverse=True)\n",
    "        self.dec = nn.Sequential(\n",
    "            *[ConvSC(C_hid, C_hid, stride=s, transpose=True) for s in strides[:-1]],\n",
    "            ConvSC(2*C_hid, C_hid, stride=strides[-1], transpose=True)\n",
    "        )\n",
    "        self.readout = nn.Conv3d(C_hid, C_out, 1)\n",
    "    \n",
    "    def forward(self, hid, enc1=None):\n",
    "        for i in range(0,len(self.dec)-1):\n",
    "            hid = self.dec[i](hid)\n",
    "        Y = self.dec[-1](torch.cat([hid, enc1], dim=1))\n",
    "        Y = self.readout(Y)\n",
    "        return Y\n",
    "\n",
    "class Mid_Xnet(nn.Module):\n",
    "    def __init__(self, channel_in, channel_hid, N_T, incep_ker = [3,5,7,11], groups=8):\n",
    "        super(Mid_Xnet, self).__init__()\n",
    "\n",
    "        self.N_T = N_T\n",
    "        enc_layers = [Inception(channel_in, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            enc_layers.append(Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "        enc_layers.append(Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "\n",
    "        dec_layers = [Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            dec_layers.append(Inception(2*channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "        dec_layers.append(Inception(2*channel_hid, channel_hid//2, channel_in, incep_ker= incep_ker, groups=groups))\n",
    "\n",
    "        self.enc = nn.Sequential(*enc_layers)\n",
    "        self.dec = nn.Sequential(*dec_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, D, H, W = x.shape\n",
    "        x = x.reshape(B, T*C, D, H, W)\n",
    "\n",
    "        # encoder\n",
    "        skips = []\n",
    "        z = x\n",
    "        for i in range(self.N_T):\n",
    "            z = self.enc[i](z)\n",
    "            if i < self.N_T - 1:\n",
    "                skips.append(z)\n",
    "\n",
    "        # decoder\n",
    "        z = self.dec[0](z)\n",
    "        for i in range(1, self.N_T):\n",
    "            z = self.dec[i](torch.cat([z, skips[-i]], dim=1))\n",
    "\n",
    "        y = z.reshape(B, T, C, D, H, W)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SimVP(nn.Module):\n",
    "    def __init__(self, shape_in, hid_S=16, hid_T=256, N_S=4, N_T=8, incep_ker=[3,5,7,11], groups=8):\n",
    "        super(SimVP, self).__init__()\n",
    "        T, C, D, H, W = shape_in\n",
    "        self.enc = Encoder(C, hid_S, N_S)\n",
    "        self.hid = Mid_Xnet(T*hid_S, hid_T, N_T, incep_ker, groups)\n",
    "        self.dec = Decoder(hid_S, C, N_S)\n",
    "\n",
    "\n",
    "    def forward(self, x_raw):\n",
    "        B, T, C, D, H, W = x_raw.shape\n",
    "        x = x_raw.view(B*T, C, D, H, W)\n",
    "\n",
    "        embed, skip = self.enc(x)\n",
    "        _, C_, D_, H_, W_ = embed.shape\n",
    "\n",
    "        z = embed.view(B, T, C_, D_, H_, W_)\n",
    "        hid = self.hid(z)\n",
    "        hid = hid.reshape(B*T, C_, D_, H_, W_)\n",
    "\n",
    "        Y = self.dec(hid, skip)\n",
    "        Y = Y.reshape(B, T, C, D, H, W)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pl Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberSSIMLoss2D(nn.Module):\n",
    "    def __init__(self, alpha=0.7, delta=0.05, window_size=5, temporal_weight=0.1):\n",
    "        super().__init__()\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # ALPHA (Controls balance between Huber Loss and SSIM)\n",
    "        # More alpha  → Model focuses on voxel accuracy (Huber loss)\n",
    "        # Less alpha  → Model prioritizes structural similarity (SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - 0.6 - 0.9 → Noisy data (CT/MRI with artifacts) → More Huber\n",
    "        # - 0.4 - 0.7 → Sharp structures (CT/MRI edges)  → Balance of both\n",
    "        # - 0.3 - 0.5 → Blurry predictions → More SSIM for finer details\n",
    "        # Default: alpha = 0.7 (Strong Huber, some SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.alpha = alpha  # More alpha = More reliance on Huber\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA (Threshold where Huber Loss switches from MSE-like to MAE-like behavior)\n",
    "        # - Higher delta → More sensitive to small errors (acts like MSE)\n",
    "        # - Lower delta  → More resistant to outliers (acts like MAE)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.05  → High-noise data (CT/MRI with artifacts) → More robust to outliers\n",
    "        # - < 0.05  → Low-noise data (Well-normalized, synthetic) → More sensitive to details\n",
    "        # - 0.02 - 0.05  → If predictions are too blurry\n",
    "        # Default: delta = 0.05 (or dynamically adjusted per epoch)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta = delta  # This will be dynamically updated every epoch\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # TEMPORAL WEIGHT (Penalizes abrupt voxel intensity changes between frames)\n",
    "        # - Higher value → Forces smoother transitions\n",
    "        # - Lower value  → Allows more flexibility in voxel changes\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.2  → Strong penalty for sudden intensity jumps (Flickering reduction)\n",
    "        # - 0.05 - 0.1  → Best for smoothly changing sequences (MRI, Weather, Fluids)\n",
    "        # - < 0.05  → Allows more dynamic changes (if model is too rigid)\n",
    "        # Default: temporal_weight = 0.1 (balanced smoothness)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.temporal_weight = temporal_weight  # More weight = Stronger smoothness enforcement\n",
    "\n",
    "        #window_size means padding\n",
    "        self.ssim_module = kornia.losses.SSIMLoss(window_size=window_size, reduction=\"mean\")\n",
    "\n",
    "    def temporal_smoothness_loss(self, y_pred):\n",
    "        #Penalizes sudden changes over time by computing L1 loss between consecutive frames.\n",
    "        #So the object should remain stationary over time\n",
    "        return torch.mean(torch.abs(y_pred[:, 1:] - y_pred[:, :-1]))  # Difference between t and t+1\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        B, T, C, H, W = y_pred.shape\n",
    "\n",
    "        # Swap time and channel dimensions: [B, C*T, H, W]\n",
    "        y_pred_restructured = y_pred.permute(0, 2, 1, 3, 4).reshape(B, C * T, H, W)\n",
    "        y_true_restructured = y_true.permute(0, 2, 1, 3, 4).reshape(B, C * T, H, W)\n",
    "\n",
    "        # Computes the SSIM treating time as another spatial dimension\n",
    "        ssim_loss = self.ssim_module(y_pred_restructured, y_true_restructured)\n",
    "\n",
    "        # Computes the Huber loss with the adjusted delta\n",
    "        huber_loss = F.huber_loss(y_pred, y_true, delta=self.delta, reduction=\"mean\")\n",
    "\n",
    "        # Compute the Temporal Smoothness Loss\n",
    "        temporal_loss = self.temporal_smoothness_loss(y_pred)\n",
    "\n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * huber_loss + (1 - self.alpha) * ssim_loss + self.temporal_weight * temporal_loss\n",
    "        return total_loss, huber_loss, ssim_loss, temporal_loss\n",
    "\n",
    "class HuberSSIMLoss3D(nn.Module):\n",
    "    def __init__(self, alpha=0.7, delta=0.05, window_size=5, temporal_weight=0.1):\n",
    "        super().__init__()\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # ALPHA (Controls balance between Huber Loss and SSIM)\n",
    "        # More alpha  → Model focuses on voxel accuracy (Huber loss)\n",
    "        # Less alpha  → Model prioritizes structural similarity (SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - 0.6 - 0.9 → Noisy data (CT/MRI with artifacts) → More Huber\n",
    "        # - 0.4 - 0.7 → Sharp structures (CT/MRI edges)  → Balance of both\n",
    "        # - 0.3 - 0.5 → Blurry predictions → More SSIM for finer details\n",
    "        # Default: alpha = 0.7 (Strong Huber, some SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.alpha = alpha  # More alpha = More reliance on Huber\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA (Threshold where Huber Loss switches from MSE-like to MAE-like behavior)\n",
    "        # - Higher delta → More sensitive to small errors (acts like MSE)\n",
    "        # - Lower delta  → More resistant to outliers (acts like MAE)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.05  → High-noise data (CT/MRI with artifacts) → More robust to outliers\n",
    "        # - < 0.05  → Low-noise data (Well-normalized, synthetic) → More sensitive to details\n",
    "        # - 0.02 - 0.05  → If predictions are too blurry\n",
    "        # Default: delta = 0.05 (or dynamically adjusted per epoch)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta = delta  # This will be dynamically updated every epoch\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # TEMPORAL WEIGHT (Penalizes abrupt voxel intensity changes between frames)\n",
    "        # - Higher value → Forces smoother transitions\n",
    "        # - Lower value  → Allows more flexibility in voxel changes\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.2  → Strong penalty for sudden intensity jumps (Flickering reduction)\n",
    "        # - 0.05 - 0.1  → Best for smoothly changing sequences (MRI, Weather, Fluids)\n",
    "        # - < 0.05  → Allows more dynamic changes (if model is too rigid)\n",
    "        # Default: temporal_weight = 0.1 (balanced smoothness)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.temporal_weight = temporal_weight  # More weight = Stronger smoothness enforcement\n",
    "\n",
    "        #window_size means padding\n",
    "        self.ssim_module = kornia.losses.SSIM3DLoss(window_size=window_size, reduction=\"mean\")\n",
    "\n",
    "    def temporal_smoothness_loss(self, y_pred):\n",
    "        #Penalizes sudden changes over time by computing L1 loss between consecutive frames.\n",
    "        #So the object should remain stationary over time\n",
    "        return torch.mean(torch.abs(y_pred[:, 1:] - y_pred[:, :-1]))  # Difference between t and t+1\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        B, T, C, D, H, W = y_pred.shape\n",
    "\n",
    "        # Swap time and channel dimensions: [B, C*T, D, H, W]\n",
    "        y_pred_restructured = y_pred.permute(0, 2, 1, 3, 4, 5).reshape(B, C * T, D, H, W)\n",
    "        y_true_restructured = y_true.permute(0, 2, 1, 3, 4, 5).reshape(B, C * T, D, H, W)\n",
    "\n",
    "        # Computes the SSIM treating time as another spatial dimension\n",
    "        ssim_loss = self.ssim_module(y_pred_restructured, y_true_restructured)\n",
    "\n",
    "        # Computes the Huber loss with the adjusted delta\n",
    "        huber_loss = F.huber_loss(y_pred, y_true, delta=self.delta, reduction=\"mean\")\n",
    "\n",
    "        # Compute the Temporal Smoothness Loss\n",
    "        temporal_loss = self.temporal_smoothness_loss(y_pred)\n",
    "\n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * huber_loss + (1 - self.alpha) * ssim_loss + self.temporal_weight * temporal_loss\n",
    "        return total_loss, huber_loss, ssim_loss, temporal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pl_Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        passed_model: nn.Module,\n",
    "        config: Dict[str, Any],\n",
    "    ):\n",
    "        super(Pl_Model, self).__init__()\n",
    "        self.passed_model = passed_model\n",
    "        self.config = config\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA FACTOR (Scales how much delta is updated per epoch)\n",
    "        # - Higher delta_factor → More aggressive updates to delta\n",
    "        # - Lower delta_factor  → Smoother, slower changes to delta\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 1.5  → If delta is too unstable (jumps too much)\n",
    "        # - 1.2 - 1.5  → Best for gradual adaptation (Default)\n",
    "        # - < 1.2  → If delta changes too slowly (use for very stable datasets)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Default: delta_factor = 1.2 (Balanced adaptation)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta_factor = 1.2  # More factor = Faster delta adjustments\n",
    "        self.delta = 0.05\n",
    "        self.previous_delta = self.delta\n",
    "        \n",
    "        #speicher alle parameter ab\n",
    "        self.save_hyperparameters(ignore=[\"passed_model\"])\n",
    "\n",
    "        # Setup training components\n",
    "        self.mse_criterion = nn.MSELoss()\n",
    "        self.psnr_criterion = PeakSignalNoiseRatio()\n",
    "        self.huberssim3d_criterion = HuberSSIMLoss3D()\n",
    "        self.huber_criterion = nn.HuberLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.passed_model(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Sets the Optimizer for the Model\"\"\"\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "        )\n",
    "        return [optimizer]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        \"\"\"Calculates the loss for a batch in different modes (training, validation, testing)\"\"\"\n",
    "        inputs, targets = batch\n",
    "\n",
    "        #forward pass\n",
    "        mse_loss = 0.0\n",
    "        huber_loss = 0.0\n",
    "        rmse_loss = 0.0\n",
    "        ssim_loss = 0.0\n",
    "        huberssim_loss = 0.0\n",
    "        temporal_loss = 0.0\n",
    "        psnr_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        for t in range(0, self.config[\"pred_frames\"], self.config[\"pred_n_frames_per_step\"]):\n",
    "            if self.config[\"pred_frames\"]-t<self.config[\"pred_n_frames_per_step\"]:\n",
    "                frames_this_step = self.config[\"pred_frames\"]-t\n",
    "            else:\n",
    "                frames_this_step = self.config[\"pred_n_frames_per_step\"]\n",
    "            outputs = self.forward(inputs)\n",
    "            #print(f\"{t}:{t+frames_this_step}\")\n",
    "            #get only the first predicted frame\n",
    "            outputs = outputs[:, :frames_this_step, :, :, :, :]\n",
    "\n",
    "            #calcualte losses\n",
    "            mse_loss_ = self.mse_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :])\n",
    "            rmse_loss_ = torch.sqrt(self.mse_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :]))\n",
    "            psnr_loss_ = self.psnr_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :])\n",
    "            huberssim_loss_, huber_loss_, ssim_loss_, temporal_loss_ = self.huberssim3d_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :])\n",
    "            total_loss_ = huberssim_loss_ #self.huber_criterion(outputs, targets[:, t:t+frames_this_step, :, :])  \n",
    "            \n",
    "            mse_loss += mse_loss_\n",
    "            huber_loss += huber_loss_\n",
    "            rmse_loss += rmse_loss_\n",
    "            ssim_loss += ssim_loss_\n",
    "            huberssim_loss += huberssim_loss_\n",
    "            temporal_loss += temporal_loss_\n",
    "            psnr_loss += psnr_loss_\n",
    "            total_loss += total_loss_\n",
    "            \n",
    "            inputs = torch.cat([inputs[:, self.config[\"pred_n_frames_per_step\"]:, :, :, :, :], outputs], dim=1)\n",
    "\n",
    "        #logging\n",
    "        self.log(f\"{mode}_mse_loss\", mse_loss)\n",
    "        self.log(f\"{mode}_huber_loss\", huber_loss)\n",
    "        self.log(f\"{mode}_rmse_loss\", rmse_loss)\n",
    "        self.log(f\"{mode}_ssim_loss\", ssim_loss)\n",
    "        self.log(f\"{mode}_huberssim_loss\", huberssim_loss)\n",
    "        self.log(f\"{mode}_temporal_loss\", temporal_loss)\n",
    "        self.log(f\"{mode}_psnr_loss\", psnr_loss)\n",
    "        self.log(f\"{mode}_total_loss\", total_loss, prog_bar=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "        inputs, targets = batch\n",
    "        outputs = []\n",
    "        for t in range(0, self.config[\"pred_frames\"], self.config[\"pred_n_frames_per_step\"]):\n",
    "            if self.config[\"pred_frames\"]-t<self.config[\"pred_n_frames_per_step\"]:\n",
    "                frames_this_step = self.config[\"pred_frames\"]-t\n",
    "            else:\n",
    "                frames_this_step = self.config[\"pred_n_frames_per_step\"]\n",
    "            outputs_t = self.forward(inputs)\n",
    "            #print(f\"{t}:{t+frames_this_step}\")\n",
    "            #get only the first predicted frame\n",
    "            outputs_t = outputs_t[:, :frames_this_step, :, :, :, :]\n",
    "            \n",
    "            outputs.append(outputs_t)\n",
    "\n",
    "            inputs = torch.cat([inputs[:, self.config[\"pred_n_frames_per_step\"]:, :, :, :, :], outputs_t], dim=1)\n",
    "            \n",
    "            #concat time and add to overall lst\n",
    "        outputs = torch.concat(outputs, dim=1)\n",
    "        \n",
    "        #calculate losses\n",
    "        mse_loss = self.mse_criterion(outputs, targets)\n",
    "        rmse_loss = torch.sqrt(self.mse_criterion(outputs, targets))\n",
    "        huberssim_loss, huber_loss, ssim_loss, temporal_loss = self.huberssim3d_criterion(outputs, targets)\n",
    "        psnr_loss = self.psnr_criterion(outputs, targets)\n",
    "        total_loss = huberssim_loss\n",
    "\n",
    "        #logging\n",
    "        self.log(f\"overall_val_mse_loss\", mse_loss)\n",
    "        self.log(f\"overall_val_huber_loss\", huber_loss)\n",
    "        self.log(f\"overall_val_rmse_loss\", rmse_loss)\n",
    "        self.log(f\"overall_val_ssim_loss\", ssim_loss)\n",
    "        self.log(f\"overall_val_huberssim_loss\", huberssim_loss)\n",
    "        self.log(f\"overall_val_temporal_loss\", temporal_loss)\n",
    "        self.log(f\"overall_val_psnr_loss\", psnr_loss)\n",
    "        self.log(f\"overall_val_total_loss\", total_loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        #adjust delta\n",
    "        val_loader = self.trainer.datamodule.val_dataloader()[0]\n",
    "        all_errors = []\n",
    "    \n",
    "        with torch.no_grad():  \n",
    "            for batch in val_loader:\n",
    "                x, y = batch\n",
    "                y_pred = self(x.to(self.device))\n",
    "                error = torch.abs(y.to(self.device) - y_pred)\n",
    "                all_errors.append(error.view(-1))\n",
    "    \n",
    "        all_errors = torch.cat(all_errors)\n",
    "        new_delta = self.delta_factor * torch.std(all_errors).item()\n",
    "\n",
    "        #Blend previous and new delta for smoother updates\n",
    "        #is capped between 0.02 and 0.35 so that is the data is too noisy huber does not just become mse\n",
    "        new_delta = min(0.5, max(0.02, 0.8 * self.previous_delta + 0.2 * new_delta))\n",
    "        self.previous_delta = new_delta\n",
    "\n",
    "        #update\n",
    "        self.huberssim3d_criterion.delta = new_delta\n",
    "    \n",
    "        #logging\n",
    "        self.log(\"delta\", new_delta)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def check_losses(self, loader, mode, use_wandb=False):\n",
    "        mse_loss = 0.0\n",
    "        huber_loss = 0.0\n",
    "        rmse_loss = 0.0\n",
    "        ssim_loss = 0.0\n",
    "        huberssim_loss = 0.0\n",
    "        temporal_loss = 0.0\n",
    "        psnr_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in loader:\n",
    "            for t in range(self.config[\"pred_frames\"]):\n",
    "                mse_loss_ = self.mse_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                huberssim_loss_, huber_loss_, ssim_loss_, temporal_loss_ = self.huberssim3d_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                rmse_loss_ = torch.sqrt(self.mse_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1)))\n",
    "                psnr_loss_ = self.psnr_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                total_loss_ = huberssim_loss_   \n",
    "                \n",
    "                mse_loss += mse_loss_.item()\n",
    "                huber_loss += huber_loss_.item()\n",
    "                rmse_loss += rmse_loss_.item()\n",
    "                ssim_loss += ssim_loss_.item()\n",
    "                huberssim_loss += huberssim_loss_.item()\n",
    "                temporal_loss += temporal_loss_.item()\n",
    "                psnr_loss += psnr_loss_.item()\n",
    "                total_loss += total_loss_.item()\n",
    "                \n",
    "        mse_loss = mse_loss / len(loader)\n",
    "        huber_loss = huber_loss / len(loader)\n",
    "        rmse_loss = rmse_loss / len(loader)\n",
    "        ssim_loss = ssim_loss / len(loader)\n",
    "        huberssim_loss = huberssim_loss / len(loader)\n",
    "        temporal_loss = temporal_loss / len(loader)\n",
    "        psnr_loss = psnr_loss / len(loader)\n",
    "        total_loss = total_loss / len(loader)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({f\"Checked_{mode}_mse_loss\": mse_loss})\n",
    "            wandb.log({f\"Checked_{mode}_huber_loss\": huber_loss})\n",
    "            wandb.log({f\"Checked_{mode}_rmse_loss\": rmse_loss})\n",
    "            wandb.log({f\"Checked_{mode}_ssim_loss\": ssim_loss})\n",
    "            wandb.log({f\"Checked_{mode}_huberssim_loss\": huberssim_loss})\n",
    "            wandb.log({f\"Checked_{mode}_temporal_loss\": temporal_loss})\n",
    "            wandb.log({f\"Checked_{mode}_psnr_loss\": psnr_loss})\n",
    "            wandb.log({f\"Checked_{mode}_total_loss\": total_loss})\n",
    "        \n",
    "        return mse_loss, huber_loss, ssim_loss, huberssim_loss, temporal_loss, rmse_loss, psnr_loss, total_loss\n",
    "        \n",
    "    def log_predictions(self):\n",
    "        \"\"\"Log example predictions to wandb\"\"\"\n",
    "        #needs to be added to other method\n",
    "        if epoch % self.config['viz_interval'] == 0:\n",
    "                self.log_predictions()\n",
    "        #but this whole method needs to be rewritten\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of validation data\n",
    "            data, target = next(iter(self.val_loader))\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            \n",
    "            # Generate predictions\n",
    "            output = self.model(data)\n",
    "            \n",
    "            # Log images\n",
    "            wandb.log({\n",
    "                \"predictions\": wandb.Image(output[0, 0].cpu()),\n",
    "                \"targets\": wandb.Image(target[0, 0].cpu()),\n",
    "                \"input_sequence\": [wandb.Image(data[0, i].cpu()) for i in range(data.shape[1])]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    #for the dataloaders\n",
    "    \"root\": \"../NormalizedQualityFiltered\",\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 1e-4,\n",
    "    \"num_workers\": 10,#0, wenn die gpu nicht benutzt wird\n",
    "    \"pin_memory\": True if torch.cuda.is_available() else False,#False, wenn die gpu nicht benutzt wird\n",
    "    \"drop_last\": False,\n",
    "    'epochs': 40,\n",
    "    #'log_interval': 20,\n",
    "    #'viz_interval': 1,\n",
    "    'run_name': '3D-SimpVP_multistep',\n",
    "    'input_frames': 9,\n",
    "    \"pred_frames\": 9,\n",
    "    \"pred_n_frames_per_step\": 1,\n",
    "    'base_filters': 32,\n",
    "    \"train_split\": 0.7,\n",
    "    \"val_split\": 0.15,\n",
    "    \"test_split\": 0.15,\n",
    "}\n",
    "config[\"run_name\"] += f\"_{config['pred_frames']}\"\n",
    "if config[\"pred_frames\"] == config[\"pred_n_frames_per_step\"]:\n",
    "    config[\"run_name\"] += \"_singleshot\"\n",
    "elif config[\"pred_n_frames_per_step\"] == 1:\n",
    "    config[\"run_name\"] += \"_autoregressive\"\n",
    "else:\n",
    "    config[\"run_name\"] += f\"_multistep_{config['pred_n_frames_per_step']}\"\n",
    "\n",
    "\n",
    "# Get data loaders\n",
    "\"\"\"train_loader, val_loader, test_loader = get_data_loaders(\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=config[\"pin_memory\"],\n",
    "    drop_last=config[\"drop_last\"],\n",
    "    sequence_length=config[\"input_frames\"], \n",
    "    prediction_length=config[\"pred_frames\"],\n",
    ")\"\"\"\n",
    "dm = VolumeDataModule(\n",
    "    root=config[\"root\"],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=config[\"pin_memory\"],\n",
    "    drop_last=config[\"drop_last\"],\n",
    "    sequence_length=config[\"input_frames\"],\n",
    "    prediction_length=config[\"pred_frames\"],\n",
    "    train_split=config[\"train_split\"],\n",
    "    val_split=config[\"val_split\"],\n",
    "    test_split=config[\"test_split\"],\n",
    ")\n",
    "wandb_logger = WandbLogger(entity=\"ChadCTP\", project=\"perfusion-ct-prediction\", name=config[\"run_name\"])\n",
    "\n",
    "# Initialize model for tuning\n",
    "model = SimVP(\n",
    "    shape_in=[config[\"input_frames\"], 1, 16, 256, 256],\n",
    "    hid_T=128,\n",
    "    hid_S=8,\n",
    ")\n",
    "\n",
    "# Initialize pl_model for tuning\n",
    "pl_model = Pl_Model(\n",
    "    passed_model=model,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_total_loss\",  \n",
    "    mode=\"min\",  \n",
    "    save_top_k=1,\n",
    "    filename=\"best-checkpoint\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer for tuning\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    accelerator=\"gpu\",\n",
    "    devices= [2] if torch.cuda.is_available() else None,\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    callbacks=[RichProgressBar(), checkpoint_callback],\n",
    "    check_val_every_n_epoch=5,\n",
    ")\n",
    "\n",
    "#wandb_logger.watch(pl_model)\n",
    "\n",
    "#tuning\n",
    "#tuner = Tuner(trainer)\n",
    "#tuner.scale_batch_size(pl_model, datamodule=dm, mode=\"binsearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250218_081321-9shklncf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/9shklncf' target=\"_blank\">3D-SimpVP_multistep_9_autoregressive</a></strong> to <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction' target=\"_blank\">https://wandb.ai/ChadCTP/perfusion-ct-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/9shklncf' target=\"_blank\">https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/9shklncf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                 </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ passed_model          │ SimVP                │ 29.3 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ mse_criterion         │ MSELoss              │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ psnr_criterion        │ PeakSignalNoiseRatio │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ huberssim3d_criterion │ HuberSSIMLoss3D      │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ huber_criterion       │ HuberLoss            │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ passed_model          │ SimVP                │ 29.3 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ mse_criterion         │ MSELoss              │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ psnr_criterion        │ PeakSignalNoiseRatio │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ huberssim3d_criterion │ HuberSSIMLoss3D      │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ huber_criterion       │ HuberLoss            │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 29.3 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 29.3 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 117                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 29.3 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 29.3 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 117                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffff138f2f084541ac4a10d3c616619f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 415: 'val_total_loss' reached inf (best inf), saving model to './perfusion-ct-prediction/9shklncf/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "Epoch 9, global step 830: 'val_total_loss' was not in top 1\n",
      "Epoch 14, global step 1245: 'val_total_loss' was not in top 1\n",
      "Epoch 19, global step 1660: 'val_total_loss' was not in top 1\n",
      "Epoch 24, global step 2075: 'val_total_loss' was not in top 1\n",
      "Epoch 29, global step 2490: 'val_total_loss' was not in top 1\n",
      "Epoch 34, global step 2905: 'val_total_loss' was not in top 1\n",
      "Epoch 39, global step 3320: 'val_total_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=pl_model,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0038747342148685434,\n",
       " 0.0013809187114116196,\n",
       " 0.386728170867029,\n",
       " nan,\n",
       " nan,\n",
       " 0.1783818316792971,\n",
       " 309.08406538712353,\n",
       " nan)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check and log the losses \"to beat\"\n",
    "dm.setup()\n",
    "pl_model.check_losses(dm.train_dataloader(), mode=\"train\", use_wandb=True)\n",
    "pl_model.check_losses(dm.val_dataloader()[0], mode=\"val\", use_wandb=True)\n",
    "pl_model.check_losses(dm.test_dataloader(), mode=\"test\", use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fa36cfe9a445c084b275dbdec446ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric       </span>┃<span style=\"font-weight: bold\">        DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   overall_val_huber_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0005628172657452524    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> overall_val_huberssim_loss </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.026806823909282684    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    overall_val_mse_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0034940876066684723    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   overall_val_psnr_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     24.57139015197754      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   overall_val_rmse_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.05909459665417671     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   overall_val_ssim_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.08671218901872635     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> overall_val_temporal_loss  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.003991898149251938    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   overall_val_total_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.026806823909282684    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_huber_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.005065354984253645    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_huberssim_loss     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        val_mse_loss        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03144676610827446     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_psnr_loss        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     220.27838134765625     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_rmse_loss        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.5316431522369385     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_ssim_loss        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.7805282473564148     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_temporal_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_total_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan             </span>│\n",
       "└────────────────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  overall_val_huber_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0005628172657452524   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36moverall_val_huberssim_loss\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.026806823909282684   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   overall_val_mse_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0034940876066684723   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  overall_val_psnr_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    24.57139015197754     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  overall_val_rmse_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.05909459665417671    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  overall_val_ssim_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.08671218901872635    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36moverall_val_temporal_loss \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.003991898149251938   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  overall_val_total_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.026806823909282684   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_huber_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.005065354984253645   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_huberssim_loss    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan            \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       val_mse_loss       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03144676610827446    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_psnr_loss       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    220.27838134765625    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_rmse_loss       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.5316431522369385    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_ssim_loss       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.7805282473564148    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_temporal_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan            \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_total_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan            \u001b[0m\u001b[35m \u001b[0m│\n",
       "└────────────────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_results = trainer.validate(pl_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4ee51c8b484e75a4d6224d7fa0d158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_huber_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.005283675156533718    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_huberssim_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_mse_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03249470144510269    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_psnr_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    219.87924194335938     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_rmse_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5400662422180176     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_ssim_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8141341805458069     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_temporal_loss     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_total_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_huber_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.005283675156533718   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_huberssim_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_mse_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03249470144510269   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_psnr_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   219.87924194335938    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_rmse_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5400662422180176    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_ssim_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8141341805458069    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_temporal_loss    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_total_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = trainer.test(pl_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_load_path = f\"../ModelWeights/{config['run_name']}.ckpt\"\n",
    "trainer.save_checkpoint(save_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n",
      "torch.Size([1, 9, 1, 16, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0034869202461681867,\n",
       " 0.0015956542368880228,\n",
       " 0.059033382487924474,\n",
       " 24.580507880763005,\n",
       " 0.04241969133420896)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def overall_loss(model, loader, device):\n",
    "    mse_loss = 0.0\n",
    "    huber_loss = 0.0\n",
    "    rmse_loss = 0.0\n",
    "    #ssim_loss = 0.0\n",
    "    psnr_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    for inputs, targets in loader:\n",
    "        print(targets.shape)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = []\n",
    "        for t in range(0, model.config[\"pred_frames\"], model.config[\"pred_n_frames_per_step\"]):\n",
    "            if model.config[\"pred_frames\"]-t<model.config[\"pred_n_frames_per_step\"]:\n",
    "                frames_this_step = model.config[\"pred_frames\"]-t\n",
    "            else:\n",
    "                frames_this_step = model.config[\"pred_n_frames_per_step\"]\n",
    "            outputs_t = model.forward(inputs)\n",
    "            #print(f\"{t}:{t+frames_this_step}\")\n",
    "            #get only the first predicted frame\n",
    "            outputs_t = outputs_t[:, :frames_this_step, :, :, :, :]\n",
    "            \n",
    "            outputs.append(outputs_t)\n",
    "\n",
    "            inputs = torch.cat([inputs[:, model.config[\"pred_n_frames_per_step\"]:, :, :, :, :], outputs_t], dim=1)\n",
    "            \n",
    "            #concat time and add to overall lst\n",
    "        outputs = torch.concat(outputs, dim=1)\n",
    "        \n",
    "        #calculate losses\n",
    "        mse_loss += model.mse_criterion(outputs, targets).item()\n",
    "        huber_loss += model.huber_criterion(outputs, targets).item()\n",
    "        rmse_loss += torch.sqrt(model.mse_criterion(outputs, targets)).item()\n",
    "        #ssim_loss = model.ssim_criterion(outputs, targets).item()\n",
    "        psnr_loss += model.psnr_criterion(outputs, targets).item()\n",
    "        total_loss += mse_loss + 0.5 * huber_loss\n",
    "\n",
    "    mse_loss = mse_loss / len(loader)\n",
    "    huber_loss = huber_loss / len(loader)\n",
    "    rmse_loss = rmse_loss / len(loader)\n",
    "    #ssim_loss = ssim_loss / len(loader)\n",
    "    psnr_loss = psnr_loss / len(loader)\n",
    "    total_loss = total_loss / len(loader)\n",
    "\n",
    "    return outputs, mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss\n",
    "\n",
    "dm.setup()\n",
    "_, mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss = overall_loss(model=pl_model, loader=dm.test_dataloader(), device=device)\n",
    "mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
