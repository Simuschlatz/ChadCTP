{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnbennewiz\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from ipywidgets import IntSlider, interact\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint\n",
    "from pytorch_lightning.tuner import Tuner\n",
    "\n",
    "import kornia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset3D(Dataset):\n",
    "    def __init__(self, data_paths, context_window=4, prediction_window=1, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.context_window = context_window\n",
    "        self.prediction_window = prediction_window\n",
    "        self.transform = transform\n",
    "\n",
    "        self.data_attributes = []\n",
    "        test = np.load(self.data_paths[0])\n",
    "        for data_path in self.data_paths:\n",
    "            for t in range(test.shape[0]-self.context_window-self.prediction_window+1):\n",
    "                # file_path, t\n",
    "                self.data_attributes.append([data_path, t])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_attributes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.data_attributes[idx][1]\n",
    "        volume_seq = torch.from_numpy(np.load(self.data_attributes[idx][0]))\n",
    "        return (\n",
    "            volume_seq[t:t+self.context_window].unsqueeze(1), \n",
    "            volume_seq[t+self.context_window:t+self.context_window+self.prediction_window].unsqueeze(1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    \"\"\"Ensures worker processes get the same seed\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class VolumeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, root, batch_size=4, sequence_length=4, prediction_length=1, num_workers=0, drop_last=False, pin_memory=False, train_split=0.8, val_split=0.1, test_split=0.1):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.pin_memory = pin_memory\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_paths = [os.path.join(self.root, path) for path in os.listdir(self.root)]\n",
    "        total_size = len(data_paths)\n",
    "    \n",
    "        # Normalize splits if they don’t sum to 1\n",
    "        split_sum = self.train_split + self.val_split + self.test_split\n",
    "        if split_sum != 1.0:\n",
    "            self.train_split /= split_sum\n",
    "            self.val_split /= split_sum\n",
    "            self.test_split /= split_sum\n",
    "            print(f\"Normalized splits to: train={self.train_split:.2f}, val={self.val_split:.2f}, test={self.test_split:.2f}\")\n",
    "    \n",
    "        # Compute dataset sizes\n",
    "        train_size = int(total_size * self.train_split)\n",
    "        val_size = int(total_size * self.val_split)\n",
    "        test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "    \n",
    "        # Error handling: Ensure valid split sizes\n",
    "        if train_size <= 0 or val_size <= 0 or test_size <= 0:\n",
    "            raise ValueError(f\"Invalid dataset splits: train={train_size}, val={val_size}, test={test_size}. Check your split values.\")\n",
    "    \n",
    "        # Perform random split\n",
    "        self.train_paths, self.val_paths, self.test_paths = random_split(data_paths, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        \n",
    "        \n",
    "        self.train_dataset = Dataset3D(self.train_paths, self.sequence_length, self.prediction_length)\n",
    "        self.val_dataset = Dataset3D(self.val_paths, self.sequence_length, self.prediction_length)\n",
    "        self.test_dataset = Dataset3D(self.test_paths, self.sequence_length, self.prediction_length)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=self.drop_last, pin_memory=self.pin_memory, worker_init_fn=seed_worker)\n",
    "\n",
    "    def teardown(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after training...\")\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after testing...\")\n",
    "\n",
    "        if stage == \"validate\" or stage is None:\n",
    "            pass\n",
    "            #print(\"Cleaning up after validation...\")\n",
    "\n",
    "        # Free memory by deleting large datasets\n",
    "        del self.train_dataset\n",
    "        del self.val_dataset\n",
    "        del self.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv3D(nn.Module):\n",
    "    \"\"\"\n",
    "    DoubleConv3D\n",
    "    -------------\n",
    "    Applies two successive 3D convolutions with Batch Normalization and ReLU activation.\n",
    "\n",
    "    Input shape: (B, in_channels, D, H, W)\n",
    "    Output shape: (B, out_channels, D, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),  # -> (B, mid_channels, D, H, W)\n",
    "            nn.BatchNorm3d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),  # -> (B, out_channels, D, H, W)\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "    \n",
    "class AdvancedTemporalBlock3D(nn.Module):\n",
    "    \"\"\"\n",
    "    AdvancedTemporalBlock3D\n",
    "    -------------------------\n",
    "    This block processes temporal relationships on volumetric features with enhanced sophistication.\n",
    "    \n",
    "    It applies a series of 1D convolutions along the temporal dimension with:\n",
    "      - Increasing dilation rates (to widen the receptive field)\n",
    "      - Residual connections (to ease optimization)\n",
    "      - Dropout (for regularization)\n",
    "      - Batch normalization and ReLU for improved training stability\n",
    "      \n",
    "    Input shape: (B, C, T, D, H, W)\n",
    "    Output shape: (B, C, T, D, H, W)  (the temporal length T is preserved)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, num_layers=2, dropout=0.2, dilation_base=2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            dilation = dilation_base ** i   # Increase dilation at each layer\n",
    "            # Set padding so that output temporal size equals input T.\n",
    "            padding = (kernel_size - 1) * dilation // 2\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(channels, channels, kernel_size=kernel_size, dilation=dilation, padding=padding),\n",
    "                    nn.BatchNorm1d(channels),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x shape: (B, C, T, D, H, W)\n",
    "        B, C, T, D, H, W = x.shape\n",
    "        \n",
    "        # Reshape to merge the spatial dimensions: (B*D*H*W, C, T)\n",
    "        x_reshaped = x.permute(0, 3, 4, 5, 1, 2).contiguous().view(-1, C, T)\n",
    "        \n",
    "        # Apply a series of temporal convolutions with residual connections.\n",
    "        for layer in self.layers:\n",
    "            out = layer(x_reshaped)  # -> (B*D*H*W, C, T)\n",
    "            out = self.dropout(out)\n",
    "            # Residual connection: the output of each layer is added to its input.\n",
    "            x_reshaped = x_reshaped + out\n",
    "        \n",
    "        # Restore the original shape:\n",
    "        # First reshape back to (B, D, H, W, C, T)\n",
    "        x_reshaped = x_reshaped.view(B, D, H, W, C, T)\n",
    "        # Permute back to (B, C, T, D, H, W)\n",
    "        out = x_reshaped.permute(0, 4, 5, 1, 2, 3).contiguous()\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class UNet3DPlusTemporal(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet3DPlusTemporal with Advanced Temporal Blocks\n",
    "    --------------------------------------------------\n",
    "    Extended UNet designed for volumetric (3D) data with temporal sequences.\n",
    "    \n",
    "    Expected input shape: (B, C, T, D, H, W)\n",
    "      B = batch size\n",
    "      C = number of input channels (e.g., 1 for grayscale)\n",
    "      T = number of time frames (must equal input_frames)\n",
    "      D, H, W = spatial dimensions\n",
    "      \n",
    "    Spatial processing is performed using 3D convolutions on each time step independently.\n",
    "    Temporal processing is then applied on the stacked features using the advanced temporal blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_frames=8, base_filters=32, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.input_frames = input_frames\n",
    "        \n",
    "        # ---------------------\n",
    "        # Encoder Path\n",
    "        # ---------------------\n",
    "        # Process each 3D volume (at a given time step) independently.\n",
    "        self.enc1 = DoubleConv3D(in_channels, base_filters)  \n",
    "        # After enc1: (B, base_filters, D, H, W)\n",
    "        self.pool1 = nn.MaxPool3d(2)  # -> (B, base_filters, D/2, H/2, W/2)\n",
    "        \n",
    "        self.enc2 = DoubleConv3D(base_filters, base_filters * 2)\n",
    "        # After enc2: (B, base_filters*2, D/2, H/2, W/2)\n",
    "        self.pool2 = nn.MaxPool3d(2)  # -> (B, base_filters*2, D/4, H/4, W/4)\n",
    "        \n",
    "        self.enc3 = DoubleConv3D(base_filters * 2, base_filters * 4)\n",
    "        # After enc3: (B, base_filters*4, D/4, H/4, W/4)\n",
    "        # Apply an advanced temporal block to capture temporal dynamics before spatial pooling.\n",
    "        self.temporal_enc = AdvancedTemporalBlock3D(channels=base_filters * 4, kernel_size=3, num_layers=2, dropout=0.2, dilation_base=2)\n",
    "        self.pool3 = nn.MaxPool3d(2)  # -> (B, base_filters*4, D/8, H/8, W/8)\n",
    "        \n",
    "        # ---------------------\n",
    "        # Bottleneck\n",
    "        # ---------------------\n",
    "        self.bottleneck_spatial = DoubleConv3D(base_filters * 4, base_filters * 8)\n",
    "        # Bottleneck features: (B, base_filters*8, D/8, H/8, W/8)\n",
    "        self.temporal_bottleneck = AdvancedTemporalBlock3D(channels=base_filters * 8, kernel_size=3, num_layers=2, dropout=0.2, dilation_base=2)\n",
    "        \n",
    "        # ---------------------\n",
    "        # Decoder Path\n",
    "        # ---------------------\n",
    "        self.upconv3 = nn.ConvTranspose3d(base_filters * 8, base_filters * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv3D(base_filters * 8, base_filters * 4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose3d(base_filters * 4, base_filters * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv3D(base_filters * 4, base_filters * 2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose3d(base_filters * 2, base_filters, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv3D(base_filters * 2, base_filters)\n",
    "        \n",
    "        self.final_conv = nn.Conv3d(base_filters, 1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of UNet3DPlusTemporal.\n",
    "        \n",
    "        x: Input tensor of shape (B, T, C, D, H, W)\n",
    "        \"\"\"\n",
    "        B, T, C, D, H, W = x.shape\n",
    "        assert T == self.input_frames, f\"Expected {self.input_frames} frames, got {T}\"\n",
    "        \n",
    "        encoder_features = []  # To store skip connections from earlier encoder stages.\n",
    "        enc3_features = []     # To store features from enc3 for temporal processing.\n",
    "        \n",
    "        # Process each time step independently through the encoder.\n",
    "        for i in range(T):\n",
    "            # Extract the 3D volume for time step i: (B, C, D, H, W)\n",
    "            curr_vol = x[:, i]\n",
    "            \n",
    "            # Encoder Stage 1\n",
    "            e1 = self.enc1(curr_vol)         # -> (B, base_filters, D, H, W)\n",
    "            p1 = self.pool1(e1)              # -> (B, base_filters, D/2, H/2, W/2)\n",
    "            \n",
    "            # Encoder Stage 2\n",
    "            e2 = self.enc2(p1)               # -> (B, base_filters*2, D/2, H/2, W/2)\n",
    "            p2 = self.pool2(e2)              # -> (B, base_filters*2, D/4, H/4, W/4)\n",
    "            \n",
    "            encoder_features.append((e1, e2))  # Save skip connection features for the decoder.\n",
    "            \n",
    "            # Encoder Stage 3\n",
    "            e3 = self.enc3(p2)               # -> (B, base_filters*4, D/4, H/4, W/4)\n",
    "            enc3_features.append(e3)\n",
    "        \n",
    "        # Stack enc3 features along the temporal dimension.\n",
    "        # New shape: (B, base_filters*4, T, D/4, H/4, W/4)\n",
    "        enc3_features = torch.stack(enc3_features, dim=2)\n",
    "        \n",
    "        # Apply advanced temporal processing.\n",
    "        enc3_processed = self.temporal_enc(enc3_features)  # -> (B, base_filters*4, T, D/4, H/4, W/4)\n",
    "        \n",
    "        # Spatial pooling across the processed features.\n",
    "        B, C3, T, D_enc, H_enc, W_enc = enc3_processed.shape\n",
    "        enc3_pooled = enc3_processed.view(B * T, C3, D_enc, H_enc, W_enc)  # (B*T, C3, D/4, H/4, W/4)\n",
    "        enc3_pooled = self.pool3(enc3_pooled)   # -> (B*T, C3, D/8, H/8, W/8)\n",
    "        _, _, D_pool, H_pool, W_pool = enc3_pooled.shape\n",
    "        enc3_pooled = enc3_pooled.view(B, C3, T, D_pool, H_pool, W_pool)\n",
    "        \n",
    "        # Bottleneck processing: apply the spatial bottleneck then advanced temporal processing for each time step.\n",
    "        bottle_features = []\n",
    "        for i in range(T):\n",
    "            feat = enc3_pooled[:, :, i]  # -> (B, C3, D/8, H/8, W/8)\n",
    "            bottle_feat = self.bottleneck_spatial(feat)  # -> (B, base_filters*8, D/8, H/8, W/8)\n",
    "            bottle_features.append(bottle_feat)\n",
    "        # Stack to get shape: (B, base_filters*8, T, D/8, H/8, W/8)\n",
    "        bottle_features = torch.stack(bottle_features, dim=2)\n",
    "        bottle_processed = self.temporal_bottleneck(bottle_features)  # -> (B, base_filters*8, T, D/8, H/8, W/8)\n",
    "        \n",
    "        # For decoding, we select the final temporal state.\n",
    "        #bottle_final = bottle_processed[:, :, -1]  # -> (B, base_filters*8, D/8, H/8, W/8)\n",
    "        \n",
    "        # ---------------------\n",
    "        # Decoder Path\n",
    "        # ---------------------\n",
    "        # Retrieve skip connection features from the last time step.\n",
    "        \n",
    "        #e1_last, e2_last = encoder_features[-1]\n",
    "        #e3_last = enc3_processed[:, :, -1]  # -> (B, base_filters*4, D/4, H/4, W/4)\n",
    "        decoder_features = []\n",
    "        for t in range(T):\n",
    "            e1_last, e2_last = encoder_features[t]\n",
    "            bottle_final = bottle_processed[:, :, t]\n",
    "            e3_last = enc3_processed[:, :, t]\n",
    "            \n",
    "            d3 = self.upconv3(bottle_final)   # -> (B, base_filters*4, D/4, H/4, W/4)\n",
    "            d3 = torch.cat([d3, e3_last], dim=1)  # Concatenated channels: (B, base_filters*8, D/4, H/4, W/4)\n",
    "            d3 = self.dec3(d3)  # -> (B, base_filters*4, D/4, H/4, W/4)\n",
    "            \n",
    "            d2 = self.upconv2(d3)             # -> (B, base_filters*2, D/2, H/2, W/2)\n",
    "            d2 = torch.cat([d2, e2_last], dim=1)   # -> (B, base_filters*4, D/2, H/2, W/2)\n",
    "            d2 = self.dec2(d2)  # -> (B, base_filters*2, D/2, H/2, W/2)\n",
    "            \n",
    "            d1 = self.upconv1(d2)             # -> (B, base_filters, D, H, W)\n",
    "            d1 = torch.cat([d1, e1_last], dim=1)   # -> (B, base_filters*2, D, H, W)\n",
    "            d1 = self.dec1(d1)  # -> (B, base_filters, D, H, W)\n",
    "            \n",
    "            d1 = self.final_conv(d1)         # -> (B, 1, D, H, W)\n",
    "            decoder_features.append(d1)\n",
    "        decoder_features = torch.stack(decoder_features, dim=1)\n",
    "        return decoder_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pl Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberSSIMLoss2D(nn.Module):\n",
    "    def __init__(self, alpha=0.7, delta=0.05, window_size=5, temporal_weight=0.1):\n",
    "        super().__init__()\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # ALPHA (Controls balance between Huber Loss and SSIM)\n",
    "        # More alpha  → Model focuses on voxel accuracy (Huber loss)\n",
    "        # Less alpha  → Model prioritizes structural similarity (SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - 0.6 - 0.9 → Noisy data (CT/MRI with artifacts) → More Huber\n",
    "        # - 0.4 - 0.7 → Sharp structures (CT/MRI edges)  → Balance of both\n",
    "        # - 0.3 - 0.5 → Blurry predictions → More SSIM for finer details\n",
    "        # Default: alpha = 0.7 (Strong Huber, some SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.alpha = alpha  # More alpha = More reliance on Huber\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA (Threshold where Huber Loss switches from MSE-like to MAE-like behavior)\n",
    "        # - Higher delta → More sensitive to small errors (acts like MSE)\n",
    "        # - Lower delta  → More resistant to outliers (acts like MAE)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.05  → High-noise data (CT/MRI with artifacts) → More robust to outliers\n",
    "        # - < 0.05  → Low-noise data (Well-normalized, synthetic) → More sensitive to details\n",
    "        # - 0.02 - 0.05  → If predictions are too blurry\n",
    "        # Default: delta = 0.05 (or dynamically adjusted per epoch)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta = delta  # This will be dynamically updated every epoch\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # TEMPORAL WEIGHT (Penalizes abrupt voxel intensity changes between frames)\n",
    "        # - Higher value → Forces smoother transitions\n",
    "        # - Lower value  → Allows more flexibility in voxel changes\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.2  → Strong penalty for sudden intensity jumps (Flickering reduction)\n",
    "        # - 0.05 - 0.1  → Best for smoothly changing sequences (MRI, Weather, Fluids)\n",
    "        # - < 0.05  → Allows more dynamic changes (if model is too rigid)\n",
    "        # Default: temporal_weight = 0.1 (balanced smoothness)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.temporal_weight = temporal_weight  # More weight = Stronger smoothness enforcement\n",
    "\n",
    "        #window_size means padding\n",
    "        self.ssim_module = kornia.losses.SSIMLoss(window_size=window_size, reduction=\"mean\")\n",
    "\n",
    "    def temporal_smoothness_loss(self, y_pred):\n",
    "        #Penalizes sudden changes over time by computing L1 loss between consecutive frames.\n",
    "        #So the object should remain stationary over time\n",
    "        return torch.mean(torch.abs(y_pred[:, 1:] - y_pred[:, :-1]))  # Difference between t and t+1\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        B, T, C, H, W = y_pred.shape\n",
    "\n",
    "        # Swap time and channel dimensions: [B, C*T, H, W]\n",
    "        y_pred_restructured = y_pred.permute(0, 2, 1, 3, 4).reshape(B, C * T, H, W)\n",
    "        y_true_restructured = y_true.permute(0, 2, 1, 3, 4).reshape(B, C * T, H, W)\n",
    "\n",
    "        # Computes the SSIM treating time as another spatial dimension\n",
    "        ssim_loss = self.ssim_module(y_pred_restructured, y_true_restructured)\n",
    "\n",
    "        # Computes the Huber loss with the adjusted delta\n",
    "        huber_loss = F.huber_loss(y_pred, y_true, delta=self.delta, reduction=\"mean\")\n",
    "\n",
    "        # Compute the Temporal Smoothness Loss\n",
    "        temporal_loss = self.temporal_smoothness_loss(y_pred)\n",
    "\n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * huber_loss + (1 - self.alpha) * ssim_loss + self.temporal_weight * temporal_loss\n",
    "        return total_loss, huber_loss, ssim_loss, temporal_loss\n",
    "\n",
    "class HuberSSIMLoss3D(nn.Module):\n",
    "    def __init__(self, alpha=0.7, delta=0.05, window_size=5, temporal_weight=0.1):\n",
    "        super().__init__()\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # ALPHA (Controls balance between Huber Loss and SSIM)\n",
    "        # More alpha  → Model focuses on voxel accuracy (Huber loss)\n",
    "        # Less alpha  → Model prioritizes structural similarity (SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - 0.6 - 0.9 → Noisy data (CT/MRI with artifacts) → More Huber\n",
    "        # - 0.4 - 0.7 → Sharp structures (CT/MRI edges)  → Balance of both\n",
    "        # - 0.3 - 0.5 → Blurry predictions → More SSIM for finer details\n",
    "        # Default: alpha = 0.7 (Strong Huber, some SSIM)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.alpha = alpha  # More alpha = More reliance on Huber\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA (Threshold where Huber Loss switches from MSE-like to MAE-like behavior)\n",
    "        # - Higher delta → More sensitive to small errors (acts like MSE)\n",
    "        # - Lower delta  → More resistant to outliers (acts like MAE)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.05  → High-noise data (CT/MRI with artifacts) → More robust to outliers\n",
    "        # - < 0.05  → Low-noise data (Well-normalized, synthetic) → More sensitive to details\n",
    "        # - 0.02 - 0.05  → If predictions are too blurry\n",
    "        # Default: delta = 0.05 (or dynamically adjusted per epoch)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta = delta  # This will be dynamically updated every epoch\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # TEMPORAL WEIGHT (Penalizes abrupt voxel intensity changes between frames)\n",
    "        # - Higher value → Forces smoother transitions\n",
    "        # - Lower value  → Allows more flexibility in voxel changes\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 0.2  → Strong penalty for sudden intensity jumps (Flickering reduction)\n",
    "        # - 0.05 - 0.1  → Best for smoothly changing sequences (MRI, Weather, Fluids)\n",
    "        # - < 0.05  → Allows more dynamic changes (if model is too rigid)\n",
    "        # Default: temporal_weight = 0.1 (balanced smoothness)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.temporal_weight = temporal_weight  # More weight = Stronger smoothness enforcement\n",
    "\n",
    "        #window_size means padding\n",
    "        self.ssim_module = kornia.losses.SSIM3DLoss(window_size=window_size, reduction=\"mean\")\n",
    "\n",
    "    def temporal_smoothness_loss(self, y_pred):\n",
    "        #Penalizes sudden changes over time by computing L1 loss between consecutive frames.\n",
    "        #So the object should remain stationary over time\n",
    "        return torch.mean(torch.abs(y_pred[:, 1:] - y_pred[:, :-1]))  # Difference between t and t+1\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        B, T, C, D, H, W = y_pred.shape\n",
    "\n",
    "        # Swap time and channel dimensions: [B, C*T, D, H, W]\n",
    "        y_pred_restructured = y_pred.permute(0, 2, 1, 3, 4, 5).reshape(B, C * T, D, H, W)\n",
    "        y_true_restructured = y_true.permute(0, 2, 1, 3, 4, 5).reshape(B, C * T, D, H, W)\n",
    "\n",
    "        # Computes the SSIM treating time as another spatial dimension\n",
    "        ssim_loss = self.ssim_module(y_pred_restructured, y_true_restructured)\n",
    "\n",
    "        # Computes the Huber loss with the adjusted delta\n",
    "        huber_loss = F.huber_loss(y_pred, y_true, delta=self.delta, reduction=\"mean\")\n",
    "\n",
    "        # Compute the Temporal Smoothness Loss\n",
    "        temporal_loss = self.temporal_smoothness_loss(y_pred)\n",
    "\n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * huber_loss + (1 - self.alpha) * ssim_loss + self.temporal_weight * temporal_loss\n",
    "        return total_loss, huber_loss, ssim_loss, temporal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pl_Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        passed_model: nn.Module,\n",
    "        config: Dict[str, Any],\n",
    "    ):\n",
    "        super(Pl_Model, self).__init__()\n",
    "        self.passed_model = passed_model\n",
    "        self.config = config\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # DELTA FACTOR (Scales how much delta is updated per epoch)\n",
    "        # - Higher delta_factor → More aggressive updates to delta\n",
    "        # - Lower delta_factor  → Smoother, slower changes to delta\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Recommended tuning:\n",
    "        # - > 1.5  → If delta is too unstable (jumps too much)\n",
    "        # - 1.2 - 1.5  → Best for gradual adaptation (Default)\n",
    "        # - < 1.2  → If delta changes too slowly (use for very stable datasets)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Default: delta_factor = 1.2 (Balanced adaptation)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        self.delta_factor = 1.2  # More factor = Faster delta adjustments\n",
    "        self.delta = 0.05\n",
    "        self.previous_delta = self.delta\n",
    "        \n",
    "        #speicher alle parameter ab\n",
    "        self.save_hyperparameters(ignore=[\"passed_model\"])\n",
    "\n",
    "        # Setup training components\n",
    "        self.mse_criterion = nn.MSELoss()\n",
    "        self.psnr_criterion = PeakSignalNoiseRatio()\n",
    "        self.huberssim3d_criterion = HuberSSIMLoss3D()\n",
    "        self.huber_criterion = nn.HuberLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.passed_model(x)\n",
    "        #Tanh has a larger gradient range, reducing saturation issues compared to sigmoid.\n",
    "        #Allows more stable gradient flow for deep networks.\n",
    "        x = 0.5*(F.tanh(x)+1)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Sets the Optimizer for the Model\"\"\"\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "        )\n",
    "        return [optimizer]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        \"\"\"Calculates the loss for a batch in different modes (training, validation, testing)\"\"\"\n",
    "        inputs, targets = batch\n",
    "\n",
    "        #forward pass\n",
    "        mse_loss = 0.0\n",
    "        huber_loss = 0.0\n",
    "        rmse_loss = 0.0\n",
    "        ssim_loss = 0.0\n",
    "        huberssim_loss = 0.0\n",
    "        temporal_loss = 0.0\n",
    "        psnr_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        for t in range(0, self.config[\"pred_frames\"], self.config[\"pred_n_frames_per_step\"]):\n",
    "            if self.config[\"pred_frames\"]-t<self.config[\"pred_n_frames_per_step\"]:\n",
    "                frames_this_step = self.config[\"pred_frames\"]-t\n",
    "            else:\n",
    "                frames_this_step = self.config[\"pred_n_frames_per_step\"]\n",
    "            outputs = self.forward(inputs)\n",
    "            #print(f\"{t}:{t+frames_this_step}\")\n",
    "            #get only the first predicted frame\n",
    "            outputs = outputs[:, :frames_this_step, :, :, :, :]\n",
    "\n",
    "            #calcualte losses\n",
    "            mse_loss_ = self.mse_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :])\n",
    "            rmse_loss_ = torch.sqrt(self.mse_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :]))\n",
    "            psnr_loss_ = self.psnr_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :])\n",
    "            huberssim_loss_, huber_loss_, ssim_loss_, temporal_loss_ = self.huberssim3d_criterion(outputs, targets[:, t:t+frames_this_step, :, :, :, :])\n",
    "            total_loss_ = huberssim_loss_ #self.huber_criterion(outputs, targets[:, t:t+frames_this_step, :, :])  \n",
    "            \n",
    "            mse_loss += mse_loss_\n",
    "            huber_loss += huber_loss_\n",
    "            rmse_loss += rmse_loss_\n",
    "            ssim_loss += ssim_loss_\n",
    "            huberssim_loss += huberssim_loss_\n",
    "            temporal_loss += temporal_loss_\n",
    "            psnr_loss += psnr_loss_\n",
    "            total_loss += total_loss_\n",
    "            \n",
    "            inputs = torch.cat([inputs[:, self.config[\"pred_n_frames_per_step\"]:, :, :, :, :], outputs], dim=1)\n",
    "\n",
    "        #logging\n",
    "        self.log(f\"{mode}_mse_loss\", mse_loss)\n",
    "        self.log(f\"{mode}_huber_loss\", huber_loss)\n",
    "        self.log(f\"{mode}_rmse_loss\", rmse_loss)\n",
    "        self.log(f\"{mode}_ssim_loss\", ssim_loss)\n",
    "        self.log(f\"{mode}_huberssim_loss\", huberssim_loss)\n",
    "        self.log(f\"{mode}_temporal_loss\", temporal_loss)\n",
    "        self.log(f\"{mode}_psnr_loss\", psnr_loss)\n",
    "        self.log(f\"{mode}_total_loss\", total_loss, prog_bar=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "        inputs, targets = batch\n",
    "        outputs = []\n",
    "        for t in range(0, self.config[\"pred_frames\"], self.config[\"pred_n_frames_per_step\"]):\n",
    "            if self.config[\"pred_frames\"]-t<self.config[\"pred_n_frames_per_step\"]:\n",
    "                frames_this_step = self.config[\"pred_frames\"]-t\n",
    "            else:\n",
    "                frames_this_step = self.config[\"pred_n_frames_per_step\"]\n",
    "            outputs_t = self.forward(inputs)\n",
    "            #print(f\"{t}:{t+frames_this_step}\")\n",
    "            #get only the first predicted frame\n",
    "            outputs_t = outputs_t[:, :frames_this_step, :, :, :, :]\n",
    "            \n",
    "            outputs.append(outputs_t)\n",
    "\n",
    "            inputs = torch.cat([inputs[:, self.config[\"pred_n_frames_per_step\"]:, :, :, :, :], outputs_t], dim=1)\n",
    "            \n",
    "            #concat time and add to overall lst\n",
    "        outputs = torch.concat(outputs, dim=1)\n",
    "        \n",
    "        #calculate losses\n",
    "        mse_loss = self.mse_criterion(outputs, targets)\n",
    "        rmse_loss = torch.sqrt(self.mse_criterion(outputs, targets))\n",
    "        huberssim_loss, huber_loss, ssim_loss, temporal_loss = self.huberssim3d_criterion(outputs, targets)\n",
    "        psnr_loss = self.psnr_criterion(outputs, targets)\n",
    "        total_loss = huberssim_loss\n",
    "\n",
    "        #logging\n",
    "        self.log(f\"overall_val_mse_loss\", mse_loss)\n",
    "        self.log(f\"overall_val_huber_loss\", huber_loss)\n",
    "        self.log(f\"overall_val_rmse_loss\", rmse_loss)\n",
    "        self.log(f\"overall_val_ssim_loss\", ssim_loss)\n",
    "        self.log(f\"overall_val_huberssim_loss\", huberssim_loss)\n",
    "        self.log(f\"overall_val_temporal_loss\", temporal_loss)\n",
    "        self.log(f\"overall_val_psnr_loss\", psnr_loss)\n",
    "        self.log(f\"overall_val_total_loss\", total_loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        #adjust delta\n",
    "        val_loader = self.trainer.datamodule.val_dataloader()\n",
    "        all_errors = []\n",
    "    \n",
    "        with torch.no_grad():  \n",
    "            for batch in val_loader:\n",
    "                x, y = batch\n",
    "                y_pred = self(x.to(self.device))\n",
    "                error = torch.abs(y.to(self.device) - y_pred)\n",
    "                all_errors.append(error.view(-1))\n",
    "    \n",
    "        all_errors = torch.cat(all_errors)\n",
    "        new_delta = self.delta_factor * torch.std(all_errors).item()\n",
    "\n",
    "        #Blend previous and new delta for smoother updates\n",
    "        #is capped between 0.02 and 0.35 so that is the data is too noisy huber does not just become mse\n",
    "        new_delta = min(0.5, max(0.02, 0.8 * self.previous_delta + 0.2 * new_delta))\n",
    "        self.previous_delta = new_delta\n",
    "\n",
    "        #update\n",
    "        self.huberssim3d_criterion.delta = new_delta\n",
    "    \n",
    "        #logging\n",
    "        self.log(\"delta\", new_delta)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def check_losses(self, loader, mode, use_wandb=False):\n",
    "        mse_loss = 0.0\n",
    "        huber_loss = 0.0\n",
    "        rmse_loss = 0.0\n",
    "        ssim_loss = 0.0\n",
    "        huberssim_loss = 0.0\n",
    "        temporal_loss = 0.0\n",
    "        psnr_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in loader:\n",
    "            for t in range(self.config[\"pred_frames\"]):\n",
    "                mse_loss_ = self.mse_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                huberssim_loss_, huber_loss_, ssim_loss_, temporal_loss_ = self.huberssim3d_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                rmse_loss_ = torch.sqrt(self.mse_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1)))\n",
    "                psnr_loss_ = self.psnr_criterion(inputs[:, -1, :, :, :].unsqueeze(1), targets[:, t, :, :].unsqueeze(1))\n",
    "                total_loss_ = huberssim_loss_   \n",
    "                \n",
    "                mse_loss += mse_loss_.item()\n",
    "                huber_loss += huber_loss_.item()\n",
    "                rmse_loss += rmse_loss_.item()\n",
    "                ssim_loss += ssim_loss_.item()\n",
    "                huberssim_loss += huberssim_loss_.item()\n",
    "                temporal_loss += temporal_loss_.item()\n",
    "                psnr_loss += psnr_loss_.item()\n",
    "                total_loss += total_loss_.item()\n",
    "                \n",
    "        mse_loss = mse_loss / len(loader)\n",
    "        huber_loss = huber_loss / len(loader)\n",
    "        rmse_loss = rmse_loss / len(loader)\n",
    "        ssim_loss = ssim_loss / len(loader)\n",
    "        huberssim_loss = huberssim_loss / len(loader)\n",
    "        temporal_loss = temporal_loss / len(loader)\n",
    "        psnr_loss = psnr_loss / len(loader)\n",
    "        total_loss = total_loss / len(loader)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({f\"Checked_{mode}_mse_loss\": mse_loss})\n",
    "            wandb.log({f\"Checked_{mode}_huber_loss\": huber_loss})\n",
    "            wandb.log({f\"Checked_{mode}_rmse_loss\": rmse_loss})\n",
    "            wandb.log({f\"Checked_{mode}_ssim_loss\": ssim_loss})\n",
    "            wandb.log({f\"Checked_{mode}_huberssim_loss\": huberssim_loss})\n",
    "            wandb.log({f\"Checked_{mode}_temporal_loss\": temporal_loss})\n",
    "            wandb.log({f\"Checked_{mode}_psnr_loss\": psnr_loss})\n",
    "            wandb.log({f\"Checked_{mode}_total_loss\": total_loss})\n",
    "        \n",
    "        return mse_loss, huber_loss, ssim_loss, huberssim_loss, temporal_loss, rmse_loss, psnr_loss, total_loss\n",
    "        \n",
    "    def log_predictions(self):\n",
    "        \"\"\"Log example predictions to wandb\"\"\"\n",
    "        #needs to be added to other method\n",
    "        if epoch % self.config['viz_interval'] == 0:\n",
    "                self.log_predictions()\n",
    "        #but this whole method needs to be rewritten\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of validation data\n",
    "            data, target = next(iter(self.val_loader))\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            \n",
    "            # Generate predictions\n",
    "            output = self.model(data)\n",
    "            \n",
    "            # Log images\n",
    "            wandb.log({\n",
    "                \"predictions\": wandb.Image(output[0, 0].cpu()),\n",
    "                \"targets\": wandb.Image(target[0, 0].cpu()),\n",
    "                \"input_sequence\": [wandb.Image(data[0, i].cpu()) for i in range(data.shape[1])]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    #for the dataloaders\n",
    "    \"root\": \"../NormalizedQualityFiltered\",\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 1e-4,\n",
    "    \"num_workers\": 10,#0, wenn die gpu nicht benutzt wird\n",
    "    \"pin_memory\": True if torch.cuda.is_available() else False,#False, wenn die gpu nicht benutzt wird\n",
    "    \"drop_last\": False,\n",
    "    'epochs': 40,\n",
    "    #'log_interval': 20,\n",
    "    #'viz_interval': 1,\n",
    "    'run_name': '3D-UNet+_temp',\n",
    "    'input_frames': 9,\n",
    "    \"pred_frames\": 9,\n",
    "    \"pred_n_frames_per_step\": 9,\n",
    "    'base_filters': 16,\n",
    "    \"train_split\": 0.7,\n",
    "    \"val_split\": 0.15,\n",
    "    \"test_split\": 0.15,\n",
    "}\n",
    "config[\"run_name\"] += f\"_{config['pred_frames']}\"\n",
    "if config[\"pred_frames\"] == config[\"pred_n_frames_per_step\"]:\n",
    "    config[\"run_name\"] += \"_NAR\"\n",
    "elif config[\"pred_n_frames_per_step\"] == 1:\n",
    "    config[\"run_name\"] += \"_FAR\"\n",
    "else:\n",
    "    config[\"run_name\"] += f\"_PAR_{config['pred_n_frames_per_step']}\"\n",
    "\n",
    "\n",
    "# Get data loaders\n",
    "\"\"\"train_loader, val_loader, test_loader = get_data_loaders(\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=config[\"pin_memory\"],\n",
    "    drop_last=config[\"drop_last\"],\n",
    "    sequence_length=config[\"input_frames\"], \n",
    "    prediction_length=config[\"pred_frames\"],\n",
    ")\"\"\"\n",
    "dm = VolumeDataModule(\n",
    "    root=config[\"root\"],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=config[\"pin_memory\"],\n",
    "    drop_last=config[\"drop_last\"],\n",
    "    sequence_length=config[\"input_frames\"],\n",
    "    prediction_length=config[\"pred_frames\"],\n",
    "    train_split=config[\"train_split\"],\n",
    "    val_split=config[\"val_split\"],\n",
    "    test_split=config[\"test_split\"],\n",
    ")\n",
    "wandb_logger = WandbLogger(entity=\"ChadCTP\", project=\"perfusion-ct-prediction\", name=config[\"run_name\"])\n",
    "\n",
    "# Initialize model for tuning\n",
    "model = UNet3DPlusTemporal(\n",
    "    input_frames=config['input_frames'],\n",
    "    base_filters=config['base_filters'],\n",
    ")\n",
    "\n",
    "# Initialize pl_model for tuning\n",
    "pl_model = Pl_Model(\n",
    "    passed_model=model,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_total_loss\",  \n",
    "    mode=\"min\",  \n",
    "    save_top_k=1,\n",
    "    filename=\"best-checkpoint\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer for tuning\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    accelerator=\"gpu\",\n",
    "    devices= [1] if torch.cuda.is_available() else None,\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    callbacks=[RichProgressBar(), checkpoint_callback],\n",
    "    check_val_every_n_epoch=5,\n",
    ")\n",
    "\n",
    "#wandb_logger.watch(pl_model)\n",
    "\n",
    "#tuning\n",
    "#tuner = Tuner(trainer)\n",
    "#tuner.scale_batch_size(pl_model, datamodule=dm, mode=\"binsearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250316_184624-uouhw2tu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/uouhw2tu' target=\"_blank\">3D-UNet+_temp_9_NAR</a></strong> to <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction' target=\"_blank\">https://wandb.ai/ChadCTP/perfusion-ct-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/uouhw2tu' target=\"_blank\">https://wandb.ai/ChadCTP/perfusion-ct-prediction/runs/uouhw2tu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                 </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ passed_model          │ UNet3DPlusTemporal   │  1.5 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ mse_criterion         │ MSELoss              │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ psnr_criterion        │ PeakSignalNoiseRatio │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ huberssim3d_criterion │ HuberSSIMLoss3D      │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ huber_criterion       │ HuberLoss            │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ passed_model          │ UNet3DPlusTemporal   │  1.5 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ mse_criterion         │ MSELoss              │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ psnr_criterion        │ PeakSignalNoiseRatio │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ huberssim3d_criterion │ HuberSSIMLoss3D      │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ huber_criterion       │ HuberLoss            │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.5 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.5 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 6                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 91                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.5 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.5 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 6                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 91                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdfcfbc26b54936b5733f51fe0ad66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 415: 'val_total_loss' reached 0.24342 (best 0.24342), saving model to './perfusion-ct-prediction/uouhw2tu/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=pl_model,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check and log the losses \"to beat\"\n",
    "dm.setup()\n",
    "pl_model.check_losses(dm.train_dataloader(), mode=\"train\", use_wandb=True)\n",
    "pl_model.check_losses(dm.val_dataloader(), mode=\"val\", use_wandb=True)\n",
    "pl_model.check_losses(dm.test_dataloader(), mode=\"test\", use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = trainer.validate(pl_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.test(pl_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_load_path = f\"../ModelWeights/{config['run_name']}.ckpt\"\n",
    "trainer.save_checkpoint(save_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "(18, 16, 256, 256)\n",
      "(18, 16, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#testing\n",
    "pl_model = Pl_Model.load_from_checkpoint(\n",
    "    \"./perfusion-ct-prediction/uouhw2tu/checkpoints/best-checkpoint.ckpt\",\n",
    "    passed_model=model,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "test_dataloader = dm.test_dataloader()\n",
    "\n",
    "print(len(test_dataloader))\n",
    "pl_model.to(device)\n",
    "\"\"\"for inputs, targets in test_dataloader:\n",
    "    outputs = pl_model.forward(inputs.to(device))\n",
    "    outputs = outputs.detach().cpu()\n",
    "    outputs = torch.concat([inputs, outputs], dim=1).squeeze(0, 2).numpy()\n",
    "    targets = torch.concat([inputs, targets], dim=1).squeeze(0, 2).numpy()\n",
    "    print(outputs.shape)\n",
    "    print(targets.shape)\n",
    "    break\"\"\"\n",
    "# method 1\n",
    "#inputs, targets = next(iter(test_dataloader))\n",
    "\n",
    "# method 2\n",
    "# only for single shot\n",
    "vol = torch.tensor(np.load(\"../NormalizedQualityFiltered/MOL-061.npy\")).unsqueeze(1)\n",
    "inputs = vol[0:9].unsqueeze(0)\n",
    "targets = vol[9:].unsqueeze(0)\n",
    "\n",
    "outputs = pl_model.forward(inputs.to(device))\n",
    "outputs = outputs.detach().cpu()\n",
    "outputs = torch.concat([inputs, outputs], dim=1).squeeze(0, 2).numpy()\n",
    "targets = torch.concat([inputs, targets], dim=1).squeeze(0, 2).numpy()\n",
    "print(outputs.shape)\n",
    "print(targets.shape)\n",
    "\n",
    "np.save(f\"outputs_{config[\"run_name\"]}.npy\", outputs)\n",
    "np.save(f\"targets_{config[\"run_name\"]}.npy\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def overall_loss(model, loader, device):\n",
    "    mse_loss = 0.0\n",
    "    huber_loss = 0.0\n",
    "    rmse_loss = 0.0\n",
    "    #ssim_loss = 0.0\n",
    "    psnr_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    for inputs, targets in loader:\n",
    "        print(targets.shape)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = []\n",
    "        for t in range(0, model.config[\"pred_frames\"], model.config[\"pred_n_frames_per_step\"]):\n",
    "            if model.config[\"pred_frames\"]-t<model.config[\"pred_n_frames_per_step\"]:\n",
    "                frames_this_step = model.config[\"pred_frames\"]-t\n",
    "            else:\n",
    "                frames_this_step = model.config[\"pred_n_frames_per_step\"]\n",
    "            outputs_t = model.forward(inputs)\n",
    "            #print(f\"{t}:{t+frames_this_step}\")\n",
    "            #get only the first predicted frame\n",
    "            outputs_t = outputs_t[:, :frames_this_step, :, :, :, :]\n",
    "            \n",
    "            outputs.append(outputs_t)\n",
    "\n",
    "            inputs = torch.cat([inputs[:, model.config[\"pred_n_frames_per_step\"]:, :, :, :, :], outputs_t], dim=1)\n",
    "            \n",
    "            #concat time and add to overall lst\n",
    "        outputs = torch.concat(outputs, dim=1)\n",
    "        \n",
    "        #calculate losses\n",
    "        mse_loss += model.mse_criterion(outputs, targets).item()\n",
    "        huber_loss += model.huber_criterion(outputs, targets).item()\n",
    "        rmse_loss += torch.sqrt(model.mse_criterion(outputs, targets)).item()\n",
    "        #ssim_loss = model.ssim_criterion(outputs, targets).item()\n",
    "        psnr_loss += model.psnr_criterion(outputs, targets).item()\n",
    "        total_loss += mse_loss + 0.5 * huber_loss\n",
    "\n",
    "    mse_loss = mse_loss / len(loader)\n",
    "    huber_loss = huber_loss / len(loader)\n",
    "    rmse_loss = rmse_loss / len(loader)\n",
    "    #ssim_loss = ssim_loss / len(loader)\n",
    "    psnr_loss = psnr_loss / len(loader)\n",
    "    total_loss = total_loss / len(loader)\n",
    "\n",
    "    return outputs, mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss\n",
    "\n",
    "dm.setup()\n",
    "_, mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss = overall_loss(model=pl_model, loader=dm.test_dataloader(), device=device)\n",
    "mse_loss, huber_loss, rmse_loss, psnr_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_vol_seq_interactive(volume_seqs, titles=None):\n",
    "    \"\"\"\n",
    "    Interactive plot of multiple volume sequences using ipywidgets\n",
    "    \n",
    "    Parameters:\n",
    "    - volume_seqs: List of 4D volume sequences to display\n",
    "    - titles: Optional list of titles for each sequence\n",
    "    \"\"\"\n",
    "    print(len(volume_seqs))\n",
    "    if titles is None:\n",
    "        titles = [f\"Volume {i+1}\" for i in range(len(volume_seqs))]\n",
    "        \n",
    "    num_volumes = len(volume_seqs)\n",
    "    nrows = int(num_volumes ** 0.5)\n",
    "    ncols = (num_volumes + nrows - 1) // nrows\n",
    "    \n",
    "    def plot_volumes(time_idx, slice_idx):\n",
    "        fig, axes = plt.subplots(nrows, ncols, \n",
    "                                figsize=(5*ncols, 5*nrows),\n",
    "                                squeeze=True)\n",
    "        if nrows == 1:\n",
    "            if ncols == 1:\n",
    "                axes = [[axes]]\n",
    "            else:\n",
    "                axes = [axes]\n",
    "                \n",
    "        for i, (volume_seq, title) in enumerate(zip(volume_seqs, titles)):\n",
    "            row, col = i // ncols, i % ncols\n",
    "            ax = axes[row][col]\n",
    "            \n",
    "            t = min(time_idx, len(volume_seq) - 1)\n",
    "            s = min(slice_idx, len(volume_seq[t]) - 1)\n",
    "            \n",
    "            im = ax.imshow(volume_seq[t][s], cmap='magma')\n",
    "            ax.set_title(title)\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show(block=True)\n",
    "        \n",
    "    max_time = max(len(vol) for vol in volume_seqs) - 1\n",
    "    max_slice = max(len(vol[0]) for vol in volume_seqs) - 1\n",
    "    \n",
    "    interact(\n",
    "        plot_volumes,\n",
    "        time_idx=IntSlider(min=0, max=max_time, step=1, value=0, description='Time:'),\n",
    "        slice_idx=IntSlider(min=0, max=max_slice, step=1, value=0, description='Slice:')\n",
    "    )\n",
    "\n",
    "multi_vol_seq_interactive([outputs, targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
