{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(path: str):\n",
    "    return np.load(path)\n",
    "\n",
    "v1 = load_data('../selected_volumes/MOL-001.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 16, 256, 256)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import IntSlider, interact\n",
    "def multi_vol_seq_interactive(volume_seqs, titles=None):\n",
    "    \"\"\"\n",
    "    Interactive plot of multiple volume sequences using ipywidgets\n",
    "    \n",
    "    Parameters:\n",
    "    - volume_seqs: List of 4D volume sequences to display\n",
    "    - titles: Optional list of titles for each sequence\n",
    "    \"\"\"\n",
    "    print(len(volume_seqs))\n",
    "    if titles is None:\n",
    "        titles = [f\"Volume {i+1}\" for i in range(len(volume_seqs))]\n",
    "        \n",
    "    num_volumes = len(volume_seqs)\n",
    "    nrows = int(num_volumes ** 0.5)\n",
    "    ncols = (num_volumes + nrows - 1) // nrows\n",
    "    \n",
    "    def plot_volumes(time_idx, slice_idx):\n",
    "        fig, axes = plt.subplots(nrows, ncols, \n",
    "                                figsize=(5*ncols, 5*nrows),\n",
    "                                squeeze=True)\n",
    "        if nrows == 1:\n",
    "            if ncols == 1:\n",
    "                axes = [[axes]]\n",
    "            else:\n",
    "                axes = [axes]\n",
    "                \n",
    "        for i, (volume_seq, title) in enumerate(zip(volume_seqs, titles)):\n",
    "            row, col = i // ncols, i % ncols\n",
    "            ax = axes[row][col]\n",
    "            \n",
    "            t = min(time_idx, len(volume_seq) - 1)\n",
    "            s = min(slice_idx, len(volume_seq[t]) - 1)\n",
    "            \n",
    "            im = ax.imshow(volume_seq[t][s], cmap='magma')\n",
    "            ax.set_title(title)\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show(block=True)\n",
    "        \n",
    "    max_time = max(len(vol) for vol in volume_seqs) - 1\n",
    "    max_slice = max(len(vol[0]) for vol in volume_seqs) - 1\n",
    "    \n",
    "    interact(\n",
    "        plot_volumes,\n",
    "        time_idx=IntSlider(min=0, max=max_time, step=1, value=0, description='Time:'),\n",
    "        slice_idx=IntSlider(min=0, max=max_slice, step=1, value=0, description='Slice:')\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045e378c22834b148997f4bb2ea9545c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Time:', max=17), IntSlider(value=0, description='Slice:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_vol_seq_interactive([v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 16, 256, 256)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps') or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1:\n",
    "- load the data as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2D(Dataset):\n",
    "    def __init__(self, data_paths, context_window=4, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.context_window = context_window\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        # For every path to a volume sequence in .npy\n",
    "        for data_path in self.data_paths:\n",
    "            volume_seq = np.load(data_path)\n",
    "            # Convert to tensor\n",
    "            volume_seq = torch.from_numpy(volume_seq)\n",
    "            for h in range(volume_seq.shape[1]):\n",
    "                # Generate samples\n",
    "                for t in range(len(volume_seq) - self.context_window):\n",
    "                    # Input volume sequence (context_window x 1 x 256 x 256), target volume (1 x 1 x 256 x 256)\n",
    "                    self.samples.append((volume_seq[t:t+self.context_window, h], \n",
    "                                         volume_seq[t+self.context_window:t+self.context_window+1, h]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2240"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = '../selected_volumes'\n",
    "data_paths = [os.path.join(root, path) for path in os.listdir(root)]\n",
    "d = Dataset2D(data_paths)\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "root = '../selected_volumes'\n",
    "device = torch.device('mps') or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Training parameters\n",
    "batch_size = 4\n",
    "sequence_length = 8\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "# Data parameters\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "\n",
    "\n",
    "data_paths = [os.path.join(root, path) for path in os.listdir(root)]\n",
    "dataset = Dataset2D(data_paths, context_window=4)\n",
    "print(len(data_paths))\n",
    "def get_data_loaders(batch_size=4, sequence_length=4):\n",
    "    # Load all folder paths\n",
    "    # Split into train/val/test\n",
    "    n_train = int(len(data_paths) * train_split)\n",
    "    n_val = int(len(data_paths) * val_split)\n",
    "    train_paths = data_paths[:n_train]\n",
    "    val_paths = data_paths[n_train:n_train+n_val]\n",
    "    test_paths = data_paths[n_train+n_val:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Dataset2D(train_paths, sequence_length)\n",
    "    val_dataset = Dataset2D(val_paths, sequence_length)\n",
    "    test_dataset = Dataset2D(test_paths, sequence_length)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders(batch_size=batch_size, sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 8, 256, 256]) torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i, (input, target) in enumerate(train_loader):\n",
    "    print(i, input.shape, target.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2Plus1D Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Testing with T=8\n",
      "Input shape: torch.Size([2, 8, 256, 256])\n",
      "Output shape: torch.Size([2, 1, 256, 256])\n",
      "Test passed!\n",
      "\n",
      "Testing with T=16\n",
      "Input shape: torch.Size([2, 16, 256, 256])\n",
      "Output shape: torch.Size([2, 1, 256, 256])\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "class DoubleConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "            \n",
    "        self.double_conv = nn.Sequential(\n",
    "            # First convolution\n",
    "            nn.Conv2d(\n",
    "                in_channels, \n",
    "                mid_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False  # No bias when using batch norm\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Second convolution\n",
    "            nn.Conv2d(\n",
    "                mid_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False  # No bias when using batch norm\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Optional: Add residual connection if input and output channels match\n",
    "        self.use_residual = in_channels == out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return self.double_conv(x) + x\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, channels, temporal_kernel_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        padding = temporal_kernel_size // 2\n",
    "        \n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            # Depthwise temporal conv\n",
    "            nn.Conv1d(channels, channels,\n",
    "                     kernel_size=temporal_kernel_size,\n",
    "                     padding=padding,\n",
    "                     groups=channels),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Point-wise conv\n",
    "            nn.Conv1d(channels, channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, T, H, W]\n",
    "        b, c, t, h, w = x.shape\n",
    "        \n",
    "        # Reshape for temporal convolution\n",
    "        x_temp = x.contiguous()  # Make memory contiguous\n",
    "        x_temp = x_temp.permute(0, 3, 4, 1, 2)  # [B, H, W, C, T]\n",
    "        x_temp = x_temp.reshape(b*h*w, c, t)\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        x_temp = self.temporal_conv(x_temp)\n",
    "        \n",
    "        # Reshape back\n",
    "        x_temp = x_temp.reshape(b, h, w, c, t)\n",
    "        x_temp = x_temp.permute(0, 3, 4, 1, 2)  # [B, C, T, H, W]\n",
    "        \n",
    "        return x_temp\n",
    "\n",
    "\n",
    "class UNet2DPlusTemporal(nn.Module):\n",
    "    def __init__(self, input_frames=None, output_frames=8, in_channels=1, base_filters=32):\n",
    "        super(UNet2DPlusTemporal, self).__init__()\n",
    "        \n",
    "        self.input_frames = input_frames\n",
    "        \n",
    "        # Update first encoder to accept in_channels instead of hardcoded 1\n",
    "        self.enc1 = DoubleConv2D(in_channels, base_filters)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = DoubleConv2D(base_filters, base_filters*2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Last encoder with temporal processing\n",
    "        self.enc3 = DoubleConv2D(base_filters*2, base_filters*4)\n",
    "        self.temporal_enc = TemporalBlock(\n",
    "            channels=base_filters*4,\n",
    "            temporal_kernel_size=3\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck with temporal processing\n",
    "        self.bottleneck_spatial = DoubleConv2D(base_filters*4, base_filters*8)\n",
    "        self.temporal_bottleneck = TemporalBlock(\n",
    "            channels=base_filters*8,\n",
    "            temporal_kernel_size=3\n",
    "        )\n",
    "        \n",
    "        # Decoder Path\n",
    "        self.upconv3 = nn.ConvTranspose2d(base_filters*8, base_filters*4, \n",
    "                                         kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv2D(base_filters*8, base_filters*4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(base_filters*4, base_filters*2, \n",
    "                                         kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv2D(base_filters*4, base_filters*2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(base_filters*2, base_filters, \n",
    "                                         kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv2D(base_filters*2, base_filters)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(base_filters, output_frames, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time, height, width]\n",
    "        b, t, c, h, w = x.shape\n",
    "        assert t == self.input_frames, f\"Expected {self.input_frames} frames, got {t}\"\n",
    "        \n",
    "        # Process each time step through initial spatial encoders\n",
    "        encoder_features = []\n",
    "        enc3_features = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            curr_frame = x[:, i]  # [B, C, H, W] - removed unsqueeze since we already have channels\n",
    "            \n",
    "            # Initial encoder path\n",
    "            e1 = self.enc1(curr_frame)\n",
    "            p1 = self.pool1(e1)\n",
    "            \n",
    "            e2 = self.enc2(p1)\n",
    "            p2 = self.pool2(e2)\n",
    "            \n",
    "            # Store for skip connections\n",
    "            encoder_features.append((e1, e2))\n",
    "            \n",
    "            # Last encoder\n",
    "            e3 = self.enc3(p2)\n",
    "            enc3_features.append(e3)\n",
    "        \n",
    "        # Process enc3 features temporally\n",
    "        enc3_features = torch.stack(enc3_features, dim=2)  # [B, C, T, H, W]\n",
    "        enc3_processed = self.temporal_enc(enc3_features)\n",
    "        \n",
    "        # Pool spatially after temporal processing\n",
    "        b, c, t, h, w = enc3_processed.shape\n",
    "        enc3_pooled = enc3_processed.contiguous()\n",
    "        enc3_pooled = enc3_pooled.view(b*t, c, h, w)\n",
    "        enc3_pooled = self.pool3(enc3_pooled)\n",
    "        _, _, h_pooled, w_pooled = enc3_pooled.shape\n",
    "        enc3_pooled = enc3_pooled.view(b, c, t, h_pooled, w_pooled)\n",
    "        \n",
    "        # Bottleneck processing\n",
    "        bottle_features = []\n",
    "        for i in range(t):\n",
    "            curr_feat = enc3_pooled[:, :, i]  # [B, C, H, W]\n",
    "            bottle_feat = self.bottleneck_spatial(curr_feat)\n",
    "            bottle_features.append(bottle_feat)\n",
    "        \n",
    "        # Stack and apply temporal processing in bottleneck\n",
    "        bottle_features = torch.stack(bottle_features, dim=2)  # [B, C, T, H, W]\n",
    "        bottle_processed = self.temporal_bottleneck(bottle_features)\n",
    "        \n",
    "        # Take last temporal state for decoder\n",
    "        bottle_final = bottle_processed[:, :, -1]  # [B, C, H, W]\n",
    "        \n",
    "        # Decoder path (using last frame's encoder features)\n",
    "        e1, e2 = encoder_features[-1]\n",
    "        e3 = enc3_processed[:, :, -1]\n",
    "        \n",
    "        d3 = self.upconv3(bottle_final)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        return self.final_conv(d1) # [B, output_frames, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/simonma/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l5unipto) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920170bb00a3468297dcbc2c00495820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▂▄▅▇█</td></tr><tr><td>batch_huber</td><td>█▂▁▁▁▁</td></tr><tr><td>batch_loss</td><td>█▂▁▁▁▁</td></tr><tr><td>batch_mse</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>100</td></tr><tr><td>batch_huber</td><td>0.04715</td></tr><tr><td>batch_loss</td><td>0.12015</td></tr><tr><td>batch_mse</td><td>0.09657</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unet2Dplus_temporal</strong> at: <a href='https://wandb.ai/simon-ma/perfusion-ct-prediction/runs/l5unipto' target=\"_blank\">https://wandb.ai/simon-ma/perfusion-ct-prediction/runs/l5unipto</a><br/> View project at: <a href='https://wandb.ai/simon-ma/perfusion-ct-prediction' target=\"_blank\">https://wandb.ai/simon-ma/perfusion-ct-prediction</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250104_174100-l5unipto/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l5unipto). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/simonma/Documents/ComputerScience/GitHub/PCTVolumePred/src/3D_Unet/wandb/run-20250104_175029-a0jsmeuu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/simon-ma/perfusion-ct-prediction/runs/a0jsmeuu' target=\"_blank\">unet2Dplus_temporal</a></strong> to <a href='https://wandb.ai/simon-ma/perfusion-ct-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/simon-ma/perfusion-ct-prediction' target=\"_blank\">https://wandb.ai/simon-ma/perfusion-ct-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/simon-ma/perfusion-ct-prediction/runs/a0jsmeuu' target=\"_blank\">https://wandb.ai/simon-ma/perfusion-ct-prediction/runs/a0jsmeuu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 185\u001b[0m\n\u001b[1;32m    181\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mtrain_with_wandb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 178\u001b[0m, in \u001b[0;36mtrain_with_wandb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WandbTrainer(\n\u001b[1;32m    170\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    171\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperfusion-ct-prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Close wandb run\u001b[39;00m\n\u001b[1;32m    181\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[55], line 106\u001b[0m, in \u001b[0;36mWandbTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(epoch)\n",
      "Cell \u001b[0;32mIn[55], line 57\u001b[0m, in \u001b[0;36mWandbTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     56\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Log batch metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.11/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.11/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.11/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class WandbTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        config: Dict[str, Any],\n",
    "        project_name: str = \"perfusion-ct-prediction\"\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize wandb\n",
    "        wandb.init(\n",
    "            project=project_name,\n",
    "            config=config,\n",
    "            name=config.get('run_name', None)\n",
    "        )\n",
    "        \n",
    "        # Setup training components\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.huber_loss = nn.HuberLoss(delta=1.0)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=config['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "    def train_epoch(self, epoch: int):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            # Move to device\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            \n",
    "            # Calculate losses\n",
    "            mse_loss = self.criterion(output, target)\n",
    "            huber_loss = self.huber_loss(output, target)\n",
    "            loss = mse_loss + 0.5 * huber_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log batch metrics\n",
    "            if batch_idx % self.config['log_interval'] == 0:\n",
    "                wandb.log({\n",
    "                    'batch': batch_idx,\n",
    "                    'batch_loss': loss.item(),\n",
    "                    'batch_mse': mse_loss.item(),\n",
    "                    'batch_huber': huber_loss.item()\n",
    "                })\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': avg_loss\n",
    "        })\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch: int):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        for data, target in self.val_loader:\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            \n",
    "            output = self.model(data)\n",
    "            val_loss += self.criterion(output, target).item()\n",
    "            \n",
    "        val_loss /= len(self.val_loader)\n",
    "        \n",
    "        # Log validation metrics\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'val_loss': val_loss\n",
    "        })\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            # Training\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self.validate(epoch)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), f\"{wandb.run.dir}/best_model.pt\")\n",
    "                wandb.save(f\"{wandb.run.dir}/best_model.pt\")\n",
    "            \n",
    "            # Log example predictions\n",
    "            if epoch % self.config['viz_interval'] == 0:\n",
    "                self.log_predictions()\n",
    "    \n",
    "    def log_predictions(self):\n",
    "        \"\"\"Log example predictions to wandb\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of validation data\n",
    "            data, target = next(iter(self.val_loader))\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            \n",
    "            # Generate predictions\n",
    "            output = self.model(data)\n",
    "            \n",
    "            # Log images\n",
    "            wandb.log({\n",
    "                \"predictions\": wandb.Image(output[0, 0].cpu()),\n",
    "                \"targets\": wandb.Image(target[0, 0].cpu()),\n",
    "                \"input_sequence\": [wandb.Image(data[0, i].cpu()) for i in range(data.shape[1])]\n",
    "            })\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def train_with_wandb():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'batch_size': 4,\n",
    "        'learning_rate': 1e-4,\n",
    "        'epochs': 10,\n",
    "        'log_interval': 20,\n",
    "        'viz_interval': 1,\n",
    "        'run_name': 'unet2Dplus_temporal',\n",
    "        'model_type': 'UNet2DPlusTemporal',\n",
    "        'input_frames': 8,\n",
    "        'output_frames': 1,\n",
    "        'base_filters': 32\n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = UNet2DPlusTemporal(\n",
    "        input_frames=config['input_frames'],\n",
    "        base_filters=config['base_filters']\n",
    "    )\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        batch_size=config['batch_size'],\n",
    "        sequence_length=config['input_frames']\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = WandbTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=config,\n",
    "        project_name=\"perfusion-ct-prediction\"\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Close wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_with_wandb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is working. 04.01.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with T=8\n",
      "Error with T=8: The operator 'aten::slow_conv3d_forward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
      "\n",
      "Testing with T=16\n",
      "Error with T=16: The operator 'aten::slow_conv3d_forward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
      "\n",
      "Testing with T=32\n",
      "Error with T=32: The operator 'aten::slow_conv3d_forward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    }
   ],
   "source": [
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            # First 3D convolution\n",
    "            nn.Conv3d(in_channels, out_channels, \n",
    "                     kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Second 3D convolution\n",
    "            nn.Conv3d(out_channels, out_channels, \n",
    "                     kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet3DTemporal(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_filters=32, input_frames=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_frames = input_frames\n",
    "        \n",
    "        # Encoder Path\n",
    "        self.enc1 = DoubleConv3D(1, base_filters)\n",
    "        # [B, 1, T, 256, 256] -> [B, 32, T, 256, 256]\n",
    "        \n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        # [B, 32, T, 256, 256] -> [B, 32, T/2, 128, 128]\n",
    "        \n",
    "        self.enc2 = DoubleConv3D(base_filters, base_filters*2)\n",
    "        # [B, 32, T/2, 128, 128] -> [B, 64, T/2, 128, 128]\n",
    "        \n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        # [B, 64, T/2, 128, 128] -> [B, 64, T/4, 64, 64]\n",
    "        \n",
    "        self.enc3 = DoubleConv3D(base_filters*2, base_filters*4)\n",
    "        # [B, 64, T/4, 64, 64] -> [B, 128, T/4, 64, 64]\n",
    "        \n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        # [B, 128, T/4, 64, 64] -> [B, 128, T/8, 32, 32]\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv3D(base_filters*4, base_filters*8)\n",
    "        # [B, 128, T/8, 32, 32] -> [B, 256, T/8, 32, 32]\n",
    "        \n",
    "        # Decoder Path\n",
    "        self.upconv3 = nn.ConvTranspose3d(\n",
    "            base_filters*8, base_filters*4,\n",
    "            kernel_size=2, stride=2\n",
    "        )\n",
    "        # [B, 256, T/8, 32, 32] -> [B, 128, T/4, 64, 64]\n",
    "        \n",
    "        self.dec3 = DoubleConv3D(base_filters*8, base_filters*4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose3d(\n",
    "            base_filters*4, base_filters*2,\n",
    "            kernel_size=2, stride=2\n",
    "        )\n",
    "        # [B, 128, T/4, 64, 64] -> [B, 64, T/2, 128, 128]\n",
    "        \n",
    "        self.dec2 = DoubleConv3D(base_filters*4, base_filters*2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose3d(\n",
    "            base_filters*2, base_filters,\n",
    "            kernel_size=2, stride=2\n",
    "        )\n",
    "        # [B, 64, T/2, 128, 128] -> [B, 32, T, 256, 256]\n",
    "        \n",
    "        self.dec1 = DoubleConv3D(base_filters*2, base_filters)\n",
    "        \n",
    "        # Final temporal reduction\n",
    "        self.final_temporal_conv = nn.Conv3d(\n",
    "            base_filters, base_filters,\n",
    "            kernel_size=(input_frames, 1, 1),\n",
    "            stride=(1, 1, 1)\n",
    "        )\n",
    "        \n",
    "        self.final_conv = nn.Conv3d(base_filters, 1, kernel_size=1)\n",
    "        \n",
    "        self.instance_norm = nn.InstanceNorm3d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input: [B, T, H, W]\n",
    "        b, t, h, w = x.shape\n",
    "        assert t == self.input_frames, f\"Expected {self.input_frames} frames, got {t}\"\n",
    "        \n",
    "        # Add channel dimension and normalize\n",
    "        x = x.unsqueeze(1)  # [B, 1, T, 256, 256]\n",
    "        x = self.instance_norm(x)\n",
    "        \n",
    "        # Encoder Path with skip connections\n",
    "        e1 = self.enc1(x)         # [B, 32, T, 256, 256]\n",
    "        p1 = self.pool1(e1)       # [B, 32, T/2, 128, 128]\n",
    "        \n",
    "        e2 = self.enc2(p1)        # [B, 64, T/2, 128, 128]\n",
    "        p2 = self.pool2(e2)       # [B, 64, T/4, 64, 64]\n",
    "        \n",
    "        e3 = self.enc3(p2)        # [B, 128, T/4, 64, 64]\n",
    "        p3 = self.pool3(e3)       # [B, 128, T/8, 32, 32]\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottle = self.bottleneck(p3)  # [B, 256, T/8, 32, 32]\n",
    "        \n",
    "        # Decoder Path\n",
    "        d3 = self.upconv3(bottle)     # [B, 128, T/4, 64, 64]\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.upconv2(d3)         # [B, 64, T/2, 128, 128]\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.upconv1(d2)         # [B, 32, T, 256, 256]\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # Final convolutions\n",
    "        out = self.final_temporal_conv(d1)  # [B, 32, 1, 256, 256]\n",
    "        out = self.final_conv(out)          # [B, 1, 1, 256, 256]\n",
    "        \n",
    "        return out.squeeze(2)  # [B, 1, 256, 256]\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    # Test with different temporal dimensions\n",
    "    temporal_sizes = [8, 16, 32]  # Must be multiples of 8 due to 3 pooling layers\n",
    "    \n",
    "    for T in temporal_sizes:\n",
    "        print(f\"\\nTesting with T={T}\")\n",
    "        model = UNet3DTemporal(input_frames=T)\n",
    "        model.to(device)\n",
    "        x = torch.randn(2, T, 256, 256)  # batch_size=2\n",
    "        \n",
    "        try:\n",
    "            out = model(x)\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "            print(f\"Output shape: {out.shape}\")\n",
    "            print(\"Test passed!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with T={T}: {str(e)}\")\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class TemporalAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, sequence_length=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm([dim, None, None])  # Normalize over channels\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time, channels, height, width]\n",
    "        b, t, c, h, w = x.shape\n",
    "        \n",
    "        # Reshape for attention\n",
    "        x = x.permute(0, 3, 4, 1, 2)  # [B, H, W, T, C]\n",
    "        x = x.reshape(b*h*w, t, c)     # [B*H*W, T, C]\n",
    "        \n",
    "        # Apply attention\n",
    "        x = self.norm(x)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Reshape back\n",
    "        x = attn_out.reshape(b, h, w, t, c)\n",
    "        x = x.permute(0, 3, 4, 1, 2)   # [B, T, C, H, W]\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class PerfusionCTPredictor(nn.Module):\n",
    "    def __init__(self, input_frames=4, base_filters=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_frames = input_frames\n",
    "        \n",
    "        # Encoder (single channel for CT scans)\n",
    "        self.enc1 = DoubleConv2D(1, base_filters)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = DoubleConv2D(base_filters, base_filters*2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck with temporal attention\n",
    "        self.bottleneck_conv = DoubleConv2D(base_filters*2, base_filters*4)\n",
    "        self.temporal_attention = TemporalAttentionBlock(\n",
    "            dim=base_filters*4,\n",
    "            num_heads=4,  # Reduced for sequence length of 4\n",
    "            sequence_length=input_frames\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv2 = nn.ConvTranspose2d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv2D(base_filters*4, base_filters*2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv2D(base_filters*2, base_filters)\n",
    "        \n",
    "        # Final prediction head\n",
    "        self.final_conv = nn.Conv2d(base_filters, 1, kernel_size=1)\n",
    "        \n",
    "        # Optional: Intensity normalization\n",
    "        self.instance_norm = nn.InstanceNorm2d(1, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time=4, height, width]\n",
    "        b, t, h, w = x.shape\n",
    "        assert t == self.input_frames, f\"Expected {self.input_frames} input frames, got {t}\"\n",
    "        \n",
    "        # Optional: Normalize intensities\n",
    "        x = x.view(b*t, 1, h, w)\n",
    "        x = self.instance_norm(x)\n",
    "        x = x.view(b, t, h, w)\n",
    "        \n",
    "        # Process each time step through encoder\n",
    "        encoder_features = []\n",
    "        bottleneck_features = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            # Encoder path\n",
    "            x1 = self.enc1(x[:, i].unsqueeze(1))  # Add channel dimension\n",
    "            x2 = self.enc2(self.pool1(x1))\n",
    "            \n",
    "            encoder_features.append((x1, x2))\n",
    "            \n",
    "            # Bottleneck\n",
    "            bottle = self.bottleneck_conv(self.pool2(x2))\n",
    "            bottleneck_features.append(bottle)\n",
    "            \n",
    "        # Stack and apply temporal attention at bottleneck\n",
    "        bottleneck_features = torch.stack(bottleneck_features, dim=1)\n",
    "        bottleneck_features = self.temporal_attention(bottleneck_features)\n",
    "        \n",
    "        # Use the last temporal feature for prediction\n",
    "        bottle = bottleneck_features[:, -1]  # Take last temporal state\n",
    "        \n",
    "        # Decoder path (single time step)\n",
    "        x1, x2 = encoder_features[-1]  # Use features from last input frame\n",
    "        \n",
    "        d2 = self.upconv2(bottle)\n",
    "        d2 = torch.cat([d2, x2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, x1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # Predict next frame\n",
    "        pred = self.final_conv(d1)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "# Training setup\n",
    "def train_perfusion_predictor():\n",
    "    model = PerfusionCTPredictor(input_frames=4)\n",
    "    criterion = nn.MSELoss()  # or nn.L1Loss()\n",
    "    \n",
    "    # Optional: Add Huber loss for robustness to outliers\n",
    "    huber_loss = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    def train_step(x, y):\n",
    "        # x: [batch, 4, height, width]\n",
    "        # y: [batch, 1, height, width] (next frame)\n",
    "        pred = model(x)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss_mse = criterion(pred, y)\n",
    "        loss_huber = huber_loss(pred, y)\n",
    "        loss = loss_mse + 0.5 * loss_huber\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D UNet with temporal Attention Block in Bootleneck"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
